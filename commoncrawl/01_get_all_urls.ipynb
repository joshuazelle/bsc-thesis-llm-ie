{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# dev_mode = True\n",
    "dev_mode = False\n",
    "if dev_mode:\n",
    "    # DEV (user specific)\n",
    "    database = \"/home/heiler/development/projects/ascii/research-space/src/pipelines/ascii/ascii_dbt/ascii_pipeline.duckdb\"\n",
    "    prefix = \"ascii_dev\"\n",
    "else:\n",
    "    # prod\n",
    "    database = \"/data/raid5/data/ascii/mastered-data/ascii_pipeline.duckdb\"\n",
    "    prefix = \"ascii\"\n",
    "\n",
    "con = duckdb.connect(\n",
    "    database=database,\n",
    "    read_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get all the urls and the ascii_id from which they came."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT e.src_url, sn.ascii_id_company\n",
    "FROM ascii_commoncrawl.edges e\n",
    "JOIN ascii_commoncrawl.seed_nodes sn ON e.src_url_surt_host = sn.seed_node_url_surt\n",
    "WHERE e.crawl_id = 'CC-MAIN-2021-49'\n",
    "AND e.seed_nodes = 'Georgetown_v1.0.0'\n",
    "AND e.src_url_surt_host NOT LIKE '%google%'\n",
    "AND sn.seeds = 'Georgetown_v1.0.0';\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urls = con.execute(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique src_urls per ascii_id_company\n",
    "url_counts = (\n",
    "    df_urls.groupby(\"ascii_id_company\")[\"src_url\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"url_count\")\n",
    ")\n",
    "\n",
    "# Calculate basic statistics\n",
    "min_urls = url_counts[\"url_count\"].min()\n",
    "max_urls = url_counts[\"url_count\"].max()\n",
    "avg_urls = url_counts[\"url_count\"].mean()\n",
    "med_urls = url_counts[\"url_count\"].median()\n",
    "\n",
    "print(f\"Minimum URLs per ID: {min_urls}\")\n",
    "print(f\"Maximum URLs per ID: {max_urls}\")\n",
    "print(f\"Average URLs per ID: {avg_urls}\")\n",
    "print(f\"Median URLs per ID: {med_urls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 75th percentile\n",
    "percentile_75 = url_counts[\"url_count\"].quantile(0.75)\n",
    "\n",
    "# Count companies with url_count above the 75th percentile\n",
    "companies_above_75th = url_counts[url_counts[\"url_count\"] > percentile_75].shape[0]\n",
    "\n",
    "print(f\"75th Percentile of URLs: {percentile_75}\")\n",
    "print(f\"Companies above 75th Percentile: {companies_above_75th}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 75th percentile\n",
    "percentile_90 = url_counts[\"url_count\"].quantile(0.9)\n",
    "\n",
    "# Count companies with url_count above the 75th percentile\n",
    "companies_above_90th = url_counts[url_counts[\"url_count\"] > percentile_90].shape[0]\n",
    "\n",
    "print(f\"90th Percentile of URLs: {percentile_90}\")\n",
    "print(f\"Companies above 90th Percentile: {companies_above_90th}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 95th percentile\n",
    "percentile_95 = url_counts[\"url_count\"].quantile(0.95)\n",
    "\n",
    "# Count companies with url_count above the 95th percentile\n",
    "companies_above_95th = url_counts[url_counts[\"url_count\"] > percentile_95].shape[0]\n",
    "\n",
    "# Calculate the 99th percentile\n",
    "percentile_99 = url_counts[\"url_count\"].quantile(0.99)\n",
    "\n",
    "# Count companies with url_count above the 99th percentile\n",
    "companies_above_99th = url_counts[url_counts[\"url_count\"] > percentile_99].shape[0]\n",
    "\n",
    "print(f\"95th Percentile of URLs: {percentile_95}\")\n",
    "print(f\"Companies above 95th Percentile: {companies_above_95th}\")\n",
    "print(f\"99th Percentile of URLs: {percentile_99}\")\n",
    "print(f\"Companies above 99th Percentile: {companies_above_99th}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distribution plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter out top 10%, 17 companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a complementary CDF plot of the URL counts for all companies with logarithmic x and y axes\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(\n",
    "    url_counts[\"url_count\"],\n",
    "    bins=np.logspace(\n",
    "        np.log10(url_counts[\"url_count\"].min()),\n",
    "        np.log10(url_counts[\"url_count\"].max()),\n",
    "        30,\n",
    "    ),\n",
    "    color=\"skyblue\",\n",
    "    edgecolor=\"black\",\n",
    "    cumulative=-1,\n",
    "    density=True,\n",
    ")\n",
    "plt.title(\"Complementary CDF of URL Counts for All Companies (Double Log Scale)\")\n",
    "plt.xlabel(\"URL Count (Log Scale)\")\n",
    "plt.ylabel(\"Complementary Cumulative Probability (Log Scale)\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "plt.xscale(\"log\")  # Set the x-axis to a logarithmic scale\n",
    "plt.yscale(\"log\")  # Set the y-axis to a logarithmic scale as well\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# 'url_counts' contains the URL count data for all companies\n",
    "\n",
    "# Fit your data to a log-normal distribution and get the parameters\n",
    "shape, loc, scale = stats.lognorm.fit(url_counts[\"url_count\"], floc=0)\n",
    "\n",
    "# Print the fitted distribution and its parameters\n",
    "print(f\"Fitted distribution: Log-normal\")\n",
    "print(f\"Shape parameter: {shape:.4f}\")\n",
    "print(f\"Location parameter: {loc:.4f}\")  # This was fixed to 0 in the fitting\n",
    "print(f\"Scale parameter: {scale:.4f}\")\n",
    "\n",
    "\n",
    "# Define the theoretical complementary CDF for the log-normal distribution\n",
    "def theoretical_compl_cdf(x, shape, scale):\n",
    "    return 1 - stats.lognorm.cdf(x, shape, loc=0, scale=scale)\n",
    "\n",
    "\n",
    "# Generate x values from min to max of your data for plotting the theoretical curve\n",
    "x = np.linspace(url_counts[\"url_count\"].min(), url_counts[\"url_count\"].max(), 1000)\n",
    "y = theoretical_compl_cdf(x, shape, scale)\n",
    "\n",
    "# Plot the theoretical complementary CDF\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(x, y, label=\"Theoretical Compl. CDF (Log-normal Fit)\", color=\"red\")\n",
    "\n",
    "# Plot the empirical complementary CDF\n",
    "plt.hist(\n",
    "    url_counts[\"url_count\"],\n",
    "    bins=np.logspace(\n",
    "        np.log10(url_counts[\"url_count\"].min()),\n",
    "        np.log10(url_counts[\"url_count\"].max()),\n",
    "        30,\n",
    "    ),\n",
    "    cumulative=-1,\n",
    "    density=True,\n",
    "    histtype=\"step\",\n",
    "    color=\"skyblue\",\n",
    "    label=\"Empirical Compl. CDF\",\n",
    ")\n",
    "\n",
    "plt.title(\"Complementary CDF of URL Counts with Theoretical Fit (Double Log Scale)\")\n",
    "plt.xlabel(\"URL Count (Log Scale)\")\n",
    "plt.ylabel(\"Complementary Cumulative Probability (Log Scale)\")\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store df_urls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ascii_rs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

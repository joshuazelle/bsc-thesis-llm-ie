{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim #for glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# dev_mode = True\n",
    "dev_mode = False\n",
    "if dev_mode:\n",
    "    # DEV (user specific)\n",
    "    database = \"/home/heiler/development/projects/ascii/research-space/src/pipelines/ascii/ascii_dbt/ascii_pipeline.duckdb\"\n",
    "    prefix = \"ascii_dev\"\n",
    "else:\n",
    "    # prod\n",
    "    database = \"/data/raid5/data/ascii/mastered-data/ascii_pipeline.duckdb\"\n",
    "    prefix = \"ascii\"\n",
    "\n",
    "con = duckdb.connect(\n",
    "    database=database,\n",
    "    read_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the URLs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting url count\n",
    "len(df_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first drop duplicated urls\n",
    "dedup_df = df_urls.drop_duplicates(subset=\"src_url\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dedup_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 filter by keywords\n",
    "\n",
    "but only if over 50 urls for a company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords = ['about', 'service', 'product', 'news', 'team', 'project', 'career']\n",
    "\n",
    "# keywords = ['about', 'service', 'product', 'news']\n",
    "\n",
    "keywords = [\"about\", \"service\", \"product\", \"news\", \"semicon\", \"technology\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply the keyword filter only to companies that have more than 50 urls, ensuring not too much information is lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify companies with more than 50 URLs\n",
    "companies_with_many_urls = (\n",
    "    dedup_df.groupby(\"ascii_id_company\")\n",
    "    .filter(lambda x: len(x) > 50)[\"ascii_id_company\"]\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "# Apply keyword filter to companies with more than 50 URLs\n",
    "keywords_pattern = \"|\".join(keywords)\n",
    "filtered_large_companies_df = dedup_df[\n",
    "    (dedup_df[\"ascii_id_company\"].isin(companies_with_many_urls))\n",
    "    & (dedup_df[\"src_url\"].str.contains(keywords_pattern, case=False, na=False))\n",
    "]\n",
    "\n",
    "# Get the data for companies with 50 or fewer URLs\n",
    "filtered_small_companies_df = dedup_df[\n",
    "    ~dedup_df[\"ascii_id_company\"].isin(companies_with_many_urls)\n",
    "]\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "filtered_df = pd.concat([filtered_large_companies_df, filtered_small_companies_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_df) / len(df_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[\"ascii_id_company\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_df[\"ascii_id_company\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok one had over 50 urls and now is lost. But it was a non existent company anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Companies with more than 50 URLs before filtering\n",
    "companies_with_many_urls_before = (\n",
    "    dedup_df.groupby(\"ascii_id_company\")\n",
    "    .filter(lambda x: len(x) > 50)[\"ascii_id_company\"]\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "# Companies present in the final filtered DataFrame\n",
    "companies_in_final_df = filtered_df[\"ascii_id_company\"].unique()\n",
    "\n",
    "# Identify lost companies\n",
    "lost_companies = set(companies_with_many_urls_before) - set(companies_in_final_df)\n",
    "\n",
    "# Retrieve the original URLs for these lost companies\n",
    "lost_companies_urls = dedup_df[dedup_df[\"ascii_id_company\"].isin(lost_companies)][\n",
    "    [\"ascii_id_company\", \"src_url\"]\n",
    "]\n",
    "\n",
    "print(\"Lost companies and their URLs:\")\n",
    "print(lost_companies_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if more than 200 urls (to not remove too many from few urls, keep median higher, not lose too much), remove longest 50% urls, assuming those go too much into detail and are not so relevant anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont do this anymore\n",
    "\n",
    "\n",
    "# def remove_longest_percent(df, percent=25, n=50):\n",
    "#     # Function to remove the longest 'percent' of URLs for each company\n",
    "#     def remove_urls(group):\n",
    "#         n_to_remove = int(len(group) * (percent / 100))\n",
    "#         if n_to_remove == 0:\n",
    "#             return group\n",
    "#         lengths = group['src_url'].str.len()\n",
    "#         threshold_length = lengths.nlargest(n_to_remove).min()\n",
    "#         return group[group['src_url'].str.len() < threshold_length]\n",
    "\n",
    "#     # Identify companies with more than n URLs\n",
    "#     companies_with_many_urls = df.groupby('ascii_id_company').filter(lambda x: len(x) > n)['ascii_id_company'].unique()\n",
    "\n",
    "#     # Apply the removal function to companies with more than n URLs\n",
    "#     filtered_large_companies_df = df[df['ascii_id_company'].isin(companies_with_many_urls)].groupby('ascii_id_company', group_keys=False).apply(remove_urls)\n",
    "\n",
    "#     # Get the data for companies with n or fewer URLs unchanged\n",
    "#     filtered_small_companies_df = df[~df['ascii_id_company'].isin(companies_with_many_urls)]\n",
    "\n",
    "#     # Concatenate the two DataFrames\n",
    "#     final_df = pd.concat([filtered_large_companies_df, filtered_small_companies_df])\n",
    "\n",
    "#     return final_df\n",
    "\n",
    "# # Apply the function with a custom percentage, e.g., 50%\n",
    "# final_df = remove_longest_percent(filtered_df, 50,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(final_df)/len(dedup_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.iloc[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short statistics on the filtered urls\n",
    "# Count unique src_urls per ascii_id_company\n",
    "url_counts = (\n",
    "    filtered_df.groupby(\"ascii_id_company\")[\"src_url\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"url_count\")\n",
    ")\n",
    "\n",
    "# Calculate basic statistics\n",
    "min_urls = url_counts[\"url_count\"].min()\n",
    "max_urls = url_counts[\"url_count\"].max()\n",
    "avg_urls = url_counts[\"url_count\"].mean()\n",
    "med_urls = url_counts[\"url_count\"].median()\n",
    "\n",
    "print(f\"Minimum URLs per ID: {min_urls}\")\n",
    "print(f\"Maximum URLs per ID: {max_urls}\")\n",
    "print(f\"Average URLs per ID: {avg_urls}\")\n",
    "print(f\"Median URLs per ID: {med_urls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter for preferrably english urls\n",
    "\n",
    "but if after this filter less than m urls would be left, dont apply it (ensures for instance firms that dont have this convention with /language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_en_urls(df, n=50, m=30):\n",
    "    # Function to filter for English URLs and revert if count falls below 'm'\n",
    "    def filter_for_company(group):\n",
    "        if len(group) > n:\n",
    "            en_filtered = group[group[\"src_url\"].str.contains(\"/en\")]\n",
    "            if len(en_filtered) >= m:\n",
    "                return en_filtered\n",
    "        return group\n",
    "\n",
    "    # Apply the filter to each company\n",
    "    filtered_df = df.groupby(\"ascii_id_company\", group_keys=False).apply(\n",
    "        filter_for_company\n",
    "    )\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# First, apply the English URL filter\n",
    "en_filtered_df = filter_en_urls(dedup_df, 50, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep only "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## next step out of these urls we will do a filtering based on similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.downloader import load\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the GloVe model\n",
    "glove_model = load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "# Define your keywords\n",
    "keywords = [\n",
    "    \"semiconductor\",\n",
    "    \"products\",\n",
    "    \"services\",\n",
    "    \"wafers\",\n",
    "    \"chips\",\n",
    "    \"silicon\",\n",
    "    \"fabrication\",\n",
    "    \"design\",\n",
    "    \"manufacturing\",\n",
    "    \"technology\",\n",
    "    \"innovation\",\n",
    "    \"electronics\",\n",
    "    \"circuit\",\n",
    "    \"transistor\",\n",
    "    \"diode\",\n",
    "    \"integrated\",\n",
    "    \"systems\",\n",
    "    \"development\",\n",
    "    \"engineering\",\n",
    "    \"industry\",\n",
    "]\n",
    "\n",
    "# Convert keywords into embeddings\n",
    "keyword_embeddings = np.array(\n",
    "    [glove_model[word] for word in keywords if word in glove_model]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_urls_by_similarity(df, n=50, m=10):\n",
    "    # Function to convert a URL into an embedding\n",
    "    def url_to_embedding(url):\n",
    "        # Split the URL by slashes and keep only the parts after the domain\n",
    "        parts = url.split(\"/\")[\n",
    "            3:\n",
    "        ]  # This excludes the 'https:', '', 'domain', and takes the rest\n",
    "        # Rejoin the parts to form the path without the domain\n",
    "        path = \"/\".join(parts)\n",
    "        # Split the path into words based on common URL delimiters like slashes and hyphens\n",
    "        words = [word for part in parts for word in part.split(\"-\")]\n",
    "        # Filter out words to only include meaningful ones (optional, based on your requirements)\n",
    "        words = [word for word in words if word and word in glove_model]\n",
    "        # Convert words into embeddings and average them to get the URL embedding\n",
    "        if words:\n",
    "            embeddings = np.array([glove_model[word] for word in words])\n",
    "            return embeddings.mean(axis=0)\n",
    "        else:\n",
    "            # Return a zero vector if no meaningful words are found\n",
    "            return np.zeros((glove_model.vector_size,))\n",
    "\n",
    "    # Function to filter URLs for a single company\n",
    "    def filter_for_company(group):\n",
    "        if len(group) <= n:\n",
    "            return group\n",
    "        else:\n",
    "            # Convert URLs to embeddings\n",
    "            url_embeddings = np.array(\n",
    "                [url_to_embedding(url) for url in group[\"src_url\"]]\n",
    "            )\n",
    "\n",
    "            # Calculate similarity to keywords\n",
    "            similarities = cosine_similarity(url_embeddings, keyword_embeddings).mean(\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            # Get indices of the top 'm' similar URLs\n",
    "            top_indices = np.argsort(similarities)[-m:]\n",
    "\n",
    "            return group.iloc[top_indices]\n",
    "\n",
    "    # Apply the filter to each company\n",
    "    filtered_df = df.groupby(\"ascii_id_company\", group_keys=False).apply(\n",
    "        filter_for_company\n",
    "    )\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to your DataFrame\n",
    "df = filter_urls_by_similarity(\n",
    "    filtered_df, 50, 70\n",
    ")  # for companies more than 50 urls, keep only the closest 70 urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for instance tesla which had issues before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df[\"ascii_id_company\"] == \"bWyO7uUNWBS9MN2QvXHLzQ==\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in df[\"src_url\"][df[\"ascii_id_company\"] == \"bWyO7uUNWBS9MN2QvXHLzQ==\"]:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in df[\"src_url\"][df[\"ascii_id_company\"] == \"+xs/sbiUV1CWPJtfrGWtMw==\"]:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in df.iloc[:50][\"src_url\"]:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urls_filtered = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store df_urls_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ascii_rs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataframe for LLM task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# dev_mode = True\n",
    "dev_mode = False\n",
    "if dev_mode:\n",
    "    # DEV (user specific)\n",
    "    database = \"/home/heiler/development/projects/ascii/research-space/src/pipelines/ascii/ascii_dbt/ascii_pipeline.duckdb\"\n",
    "    prefix = \"ascii_dev\"\n",
    "else:\n",
    "    # prod\n",
    "    database = \"/data/raid5/data/ascii/mastered-data/ascii_pipeline.duckdb\"\n",
    "    prefix = \"ascii\"\n",
    "\n",
    "con = duckdb.connect(\n",
    "    database=database,\n",
    "    read_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df_extr_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        # Detect the language of the text\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        # Return a placeholder if language detection fails\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "# Apply the language detection function to the 'extr_text' column\n",
    "df_extr_text[\"language\"] = df_extr_text[\"extr_text\"].apply(detect_language)\n",
    "\n",
    "# Create a count table for the languages\n",
    "language_counts = df_extr_text[\"language\"].value_counts()\n",
    "\n",
    "# Display the language count table\n",
    "language_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT * \n",
    "    FROM READ_CSV('/data/raid5/data/ascii/mastered-data/reference-data/data_raw_direct_source_drop/joshua/georgetown/provision.csv', HEADER=TRUE);\n",
    "    \"\"\"\n",
    "provision = con.execute(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich\n",
    "\n",
    "rich.print(provision.iloc[6:11, 0:4].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT input_id, input_name, type, stage_name, stage_id, description\n",
    "    FROM READ_CSV('/data/raid5/data/ascii/mastered-data/reference-data/data_raw_direct_source_drop/joshua/georgetown/inputs.csv', HEADER=TRUE);\n",
    "    \"\"\"\n",
    "input_desc = con.execute(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_desc.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_desc.iloc[3:8, :5].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    acsr.ascii_id_company,\n",
    "    csv.provider_id,\n",
    "    csv.provider_name,\n",
    "    csv.provided_name,\n",
    "    csv.provided_id\n",
    "    \n",
    "FROM\n",
    "    READ_CSV('/data/raid5/data/ascii/mastered-data/reference-data/data_raw_direct_source_drop/joshua/georgetown/provision.csv', HEADER=TRUE) AS csv\n",
    "JOIN\n",
    "    ascii.company_source_rel AS acsr\n",
    "ON\n",
    "    csv.provider_id = acsr.id_number\n",
    "WHERE\n",
    "    acsr.id_number_type = 'georgetown_id';\n",
    "\"\"\"\n",
    "\n",
    "provision = con.execute(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provision[\"ascii_id_company\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provision[provision[\"provider_name\"] == \"TSMC\"]  # example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "362 / 232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(provision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(provision) / 79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provision[\"provided_name\"].value_counts().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection\n",
    "\n",
    "So the thing is that there are types which have material_resource, tool_resource and process. Then the process has 3 more subcategories, namely design, fabrication and Assembly, Testing, Packaging (ATP)\n",
    "\n",
    "I just want to predict these stages.\n",
    "\n",
    "ANd why did i do this? Because for instance TSMC would otherwise just have ATP, Fabrication and photomask, and then just photomask, and who would classify tsmc as photomask.\n",
    "\n",
    "so now assign the class for these 5 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov_in = pd.merge(\n",
    "    provision, input_desc, how=\"left\", left_on=\"provided_id\", right_on=\"input_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_class(row):\n",
    "    if pd.notna(row[\"stage_name\"]):\n",
    "        return row[\"stage_name\"]\n",
    "    elif pd.notna(row[\"type\"]):\n",
    "        return row[\"type\"]\n",
    "    else:\n",
    "        return row[\"provided_name\"]\n",
    "\n",
    "\n",
    "prov_in[\"class\"] = prov_in.apply(assign_class, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prov_in[\"class\"].value_counts().to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov_in = prov_in[\n",
    "    [\"ascii_id_company\", \"provider_name\", \"provided_name\", \"provided_id\", \"class\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prov_in.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provision_transformed = (\n",
    "    prov_in.groupby([\"ascii_id_company\", \"provider_name\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"provided_name\": lambda x: list(set(x)),\n",
    "            \"provided_id\": lambda x: list(set(x)),\n",
    "            \"class\": lambda x: list(set(x)),\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check new distribution\n",
    "\n",
    "# Flatten the list of classes from all rows into a single list\n",
    "all_classes = provision_transformed[\"class\"].explode()\n",
    "\n",
    "# Use value_counts() on the flattened list to get the distribution\n",
    "class_distribution = all_classes.value_counts()\n",
    "\n",
    "# Display the class distribution\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_distribution.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Georgetown with Orbis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provision_transformed.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(provision_transformed.iloc[:10][[\"provider_name\", \"class\"]].to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.query(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.query(\"pragma table_info(ascii_ref_clean.orbis_company_trade_description)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what looks promising here is products_services, trade_description_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.query(\"pragma table_info(ascii_ref_clean.orbis_company_overview)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok what looks promising here is main_products_and_services, main_activity, full_overview, primary_business_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get these columns for our companies\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    acsr.ascii_id_company,\n",
    "    ov.main_products_and_services, \n",
    "    ov.main_activity, \n",
    "    ov.full_overview, \n",
    "    ov.primary_business_line\n",
    "FROM \n",
    "    ascii.company_source_rel AS acsr\n",
    "LEFT JOIN\n",
    "    ascii_ref_clean.orbis_company_overview ov\n",
    "ON\n",
    "    acsr.ascii_id_company = ov.ascii_id_company\n",
    "WHERE\n",
    "    acsr.id_number_type = 'georgetown_id';\n",
    "\"\"\"\n",
    "overview = con.execute(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get these columns for our companies\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    acsr.ascii_id_company,\n",
    "    trade.products_services, \n",
    "    trade.trade_description_english\n",
    "FROM \n",
    "    ascii.company_source_rel AS acsr\n",
    "LEFT JOIN\n",
    "    ascii_ref_clean.orbis_company_trade_description trade\n",
    "ON\n",
    "    acsr.ascii_id_company = trade.ascii_id_company\n",
    "WHERE\n",
    "    acsr.id_number_type = 'georgetown_id';\n",
    "\"\"\"\n",
    "trade = con.execute(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbis = pd.merge(trade, overview, on=\"ascii_id_company\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are quite a few companies that dont have any description in orbis. Drop those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbis = orbis[\n",
    "    [\n",
    "        \"ascii_id_company\",\n",
    "        \"main_products_and_services\",\n",
    "        \"full_overview\",\n",
    "        \"primary_business_line\",\n",
    "        \"trade_description_english\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep rows with at least 1 non-NA value in columns other than 'ascii_id_company'\n",
    "orbis = orbis.dropna(thresh=2, subset=orbis.columns.difference([\"ascii_id_company\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(orbis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now merge orbis with georgetown\n",
    "gt_orb = pd.merge(provision_transformed, orbis, on=\"ascii_id_company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_orb.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all description into one string\n",
    "gt_orb[\"orbis_description\"] = gt_orb.apply(\n",
    "    lambda row: \"'main_products_and_services':'{}', 'full_overview':'{}', 'primary_business_line':'{}', 'trade_description_english':'{}'\".format(\n",
    "        row[\"main_products_and_services\"],\n",
    "        row[\"full_overview\"],\n",
    "        row[\"primary_business_line\"],\n",
    "        row[\"trade_description_english\"],\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Drop the original columns\n",
    "gt_orb = gt_orb.drop(\n",
    "    columns=[\n",
    "        \"main_products_and_services\",\n",
    "        \"full_overview\",\n",
    "        \"primary_business_line\",\n",
    "        \"trade_description_english\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_orb.iloc[1, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the string length\n",
    "gt_orb[\"description_length\"] = gt_orb[\"orbis_description\"].str.len()\n",
    "\n",
    "# Sort the DataFrame by 'description_length' in descending order\n",
    "gt_orb = gt_orb.sort_values(by=\"description_length\", ascending=False)\n",
    "\n",
    "gt_orb = gt_orb.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_orb = gt_orb.rename({\"orbis_description\": \"text\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gt_orb.iloc[39][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_orb[\"description_length\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_orb[[\"provider_name\", \"text\", \"class\"]].iloc[5:10].to_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate for latex\n",
    "\n",
    "\n",
    "# Define a function to truncate text\n",
    "def truncate_text(text, max_length=40):\n",
    "    return text if len(text) <= max_length else text[: max_length - 3] + \"...\"\n",
    "\n",
    "\n",
    "# Apply the truncation to the DataFrame\n",
    "gt_orb_truncated = gt_orb.applymap(\n",
    "    lambda x: truncate_text(x, 40) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "# Export to LaTeX\n",
    "latex_table = (\n",
    "    gt_orb_truncated[[\"provider_name\", \"text\", \"class\"]]\n",
    "    .iloc[5:10]\n",
    "    .to_latex(index=False)\n",
    ")\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store as file for .py\n",
    "import pickle\n",
    "\n",
    "with open(\n",
    "    \"/home/zelle/development/projects/ascii/reference-data/data_raw_direct_source_drop/joshua/llm_data/gt_orb.pickle\",\n",
    "    \"wb\",\n",
    ") as f:\n",
    "    pickle.dump(gt_orb, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quite reasonable lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Now we could use this directly or use only the companies found also in commoncrawl..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Georgetown with CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with CC data\n",
    "df_cc = pd.merge(df_extr_text, provision_transformed, on=\"ascii_id_company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_extr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store df_cc\n",
    "%store input_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how much gpt-4 would cost for CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_cc[\"extr_text\"].str.len().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100000 * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the extr text to text for standardization\n",
    "df_cc.rename({\"extr_text\": \"text\"}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store as file for .py\n",
    "import pickle\n",
    "\n",
    "with open(\n",
    "    \"/home/zelle/development/projects/ascii/reference-data/data_raw_direct_source_drop/joshua/llm_data/df_cc.pickle\",\n",
    "    \"wb\",\n",
    ") as f:\n",
    "    pickle.dump(df_cc, f)\n",
    "\n",
    "with open(\n",
    "    \"/home/zelle/development/projects/ascii/reference-data/data_raw_direct_source_drop/joshua/llm_data/input_desc.pickle\",\n",
    "    \"wb\",\n",
    ") as f:\n",
    "    pickle.dump(input_desc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cc.iloc[35, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ascii_rs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

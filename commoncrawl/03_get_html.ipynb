{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# dev_mode = True\n",
    "dev_mode = False\n",
    "if dev_mode:\n",
    "    # DEV (user specific)\n",
    "    database = \"/home/heiler/development/projects/ascii/research-space/src/pipelines/ascii/ascii_dbt/ascii_pipeline.duckdb\"\n",
    "    prefix = \"ascii_dev\"\n",
    "else:\n",
    "    # prod\n",
    "    database = \"/data/raid5/data/ascii/mastered-data/ascii_pipeline.duckdb\"\n",
    "    prefix = \"ascii\"\n",
    "\n",
    "con = duckdb.connect(\n",
    "    database=database,\n",
    "    read_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df_urls_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_urls_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ascii_id_company\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve content from remaining links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a temporary table for the URLs and ascii_id_company\n",
    "# create_temp_table_query = \"CREATE TEMPORARY TABLE temp_urls (src_url VARCHAR, ascii_id_company VARCHAR)\"\n",
    "# con.execute(create_temp_table_query)\n",
    "\n",
    "# # Insert the URLs and ascii_id_company from processed_df into the temporary table\n",
    "# for _, row in df.iterrows():\n",
    "#     insert_query = f\"INSERT INTO temp_urls VALUES ('{row['src_url']}', '{row['ascii_id_company']}')\"\n",
    "#     con.execute(insert_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary table for the URLs and ascii_id_company\n",
    "create_temp_table_query = (\n",
    "    \"CREATE TEMPORARY TABLE temp_urls (src_url VARCHAR, ascii_id_company VARCHAR)\"\n",
    ")\n",
    "con.execute(create_temp_table_query)\n",
    "\n",
    "# Insert the URLs and ascii_id_company from processed_df into the temporary table\n",
    "for _, row in df.iterrows():\n",
    "    insert_query = \"INSERT INTO temp_urls (src_url, ascii_id_company) VALUES (?, ?)\"\n",
    "    con.execute(insert_query, (row[\"src_url\"], row[\"ascii_id_company\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.query(\"select * from temp_urls limit 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to join the temporary table with edges and get content along with ascii_id_company\n",
    "content_query = \"\"\"\n",
    "SELECT t.ascii_id_company, t.src_url, e.content AS content\n",
    "FROM temp_urls t\n",
    "JOIN ascii_commoncrawl.edges e ON t.src_url = e.src_url\n",
    "WHERE e.seed_nodes = 'Georgetown_v1.0.0' AND e.crawl_id = 'CC-MAIN-2021-49'\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Execute the query and fetch the result into a DataFrame WARNING takes long time! around 1m\n",
    "content_df = con.execute(content_query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(content_df))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok we got some duplicated urls, those we will drop and just keep the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df = content_df.drop_duplicates(subset=\"src_url\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(content_df))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store content_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ascii_rs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

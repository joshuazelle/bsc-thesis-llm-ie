{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim #for glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# dev_mode = True\n",
    "dev_mode = False\n",
    "if dev_mode:\n",
    "    # DEV (user specific)\n",
    "    database = \"/home/heiler/development/projects/ascii/research-space/src/pipelines/ascii/ascii_dbt/ascii_pipeline.duckdb\"\n",
    "    prefix = \"ascii_dev\"\n",
    "else:\n",
    "    # prod\n",
    "    database = \"/data/raid5/data/ascii/mastered-data/ascii_pipeline.duckdb\"\n",
    "    prefix = \"ascii\"\n",
    "\n",
    "con = duckdb.connect(\n",
    "    database=database,\n",
    "    read_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the URLs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting url count\n",
    "len(df_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first drop duplicated urls\n",
    "dedup_df = df_urls.drop_duplicates(subset=\"src_url\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dedup_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 filter by keywords\n",
    "\n",
    "but only if over 50 urls for a company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords = ['about', 'service', 'product', 'news', 'team', 'project', 'career']\n",
    "\n",
    "# keywords = ['about', 'service', 'product', 'news']\n",
    "\n",
    "keywords = [\"about\", \"service\", \"product\", \"news\", \"semicon\", \"technology\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply the keyword filter only to companies that have more than 50 urls, ensuring not too much information is lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify companies with more than 50 URLs\n",
    "companies_with_many_urls = (\n",
    "    dedup_df.groupby(\"ascii_id_company\")\n",
    "    .filter(lambda x: len(x) > 50)[\"ascii_id_company\"]\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "# Apply keyword filter to companies with more than 50 URLs\n",
    "keywords_pattern = \"|\".join(keywords)\n",
    "filtered_large_companies_df = dedup_df[\n",
    "    (dedup_df[\"ascii_id_company\"].isin(companies_with_many_urls))\n",
    "    & (dedup_df[\"src_url\"].str.contains(keywords_pattern, case=False, na=False))\n",
    "]\n",
    "\n",
    "# Get the data for companies with 50 or fewer URLs\n",
    "filtered_small_companies_df = dedup_df[\n",
    "    ~dedup_df[\"ascii_id_company\"].isin(companies_with_many_urls)\n",
    "]\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "filtered_df = pd.concat([filtered_large_companies_df, filtered_small_companies_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_df) / len(dedup_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[\"ascii_id_company\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_df[\"ascii_id_company\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short statistics on the filtered urls\n",
    "# Count unique src_urls per ascii_id_company\n",
    "url_counts = (\n",
    "    filtered_df.groupby(\"ascii_id_company\")[\"src_url\"]\n",
    "    .nunique()\n",
    "    .reset_index(name=\"url_count\")\n",
    ")\n",
    "\n",
    "# Calculate basic statistics\n",
    "min_urls = url_counts[\"url_count\"].min()\n",
    "max_urls = url_counts[\"url_count\"].max()\n",
    "avg_urls = url_counts[\"url_count\"].mean()\n",
    "med_urls = url_counts[\"url_count\"].median()\n",
    "\n",
    "print(f\"Minimum URLs per ID: {min_urls}\")\n",
    "print(f\"Maximum URLs per ID: {max_urls}\")\n",
    "print(f\"Average URLs per ID: {avg_urls}\")\n",
    "print(f\"Median URLs per ID: {med_urls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter for preferrably english urls\n",
    "\n",
    "but if after this filter less than m urls would be left, dont apply it (ensures for instance firms that dont have this convention with /language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_en_urls(df, n=50, m=30):\n",
    "    # Function to filter for English URLs and revert if count falls below 'm'\n",
    "    def filter_for_company(group):\n",
    "        if len(group) > n:\n",
    "            en_filtered = group[group[\"src_url\"].str.contains(\"/en\")]\n",
    "            if len(en_filtered) >= m:\n",
    "                return en_filtered\n",
    "        return group\n",
    "\n",
    "    # Apply the filter to each company\n",
    "    filtered_df = df.groupby(\"ascii_id_company\", group_keys=False).apply(\n",
    "        filter_for_company\n",
    "    )\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# First, apply the English URL filter\n",
    "en_filtered_df = filter_en_urls(\n",
    "    dedup_df, 100, 200\n",
    ")  # if company has more than 100 urls, then apply and if falls under 200 urls revert change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep only "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## next step out of these urls we will do a filtering based on similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.downloader import load\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the GloVe model\n",
    "glove_model = load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the descriptions of the input steps from georgtetown as the similarity search query to compare against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT description\n",
    "    FROM READ_CSV('/data/raid5/data/ascii/mastered-data/reference-data/data_raw_direct_source_drop/joshua/georgetown/inputs.csv', HEADER=TRUE);\n",
    "    \"\"\"\n",
    "input_desc = con.execute(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Assuming 'input_desc' is your DataFrame and 'description' is the column with text\n",
    "# Step 1: Concatenate all descriptions into a single text\n",
    "all_descriptions = \" \".join(input_desc[\"description\"])\n",
    "\n",
    "# Step 2: Clean the text\n",
    "cleaned_text = re.sub(r\"[^a-zA-Z\\s]\", \"\", all_descriptions)\n",
    "cleaned_text = cleaned_text.lower()\n",
    "\n",
    "# Step 3: Remove stop words\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "word_tokens = word_tokenize(cleaned_text)\n",
    "filtered_text = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "# Step 4: Lemmatize\n",
    "nltk.download(\"wordnet\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_text = [lemmatizer.lemmatize(w) for w in filtered_text]\n",
    "\n",
    "# The 'lemmatized_text' now contains your processed text ready for GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = np.array([glove_model[word] for word in lemmatized_text if word in glove_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def filter_urls_by_similarity(df, n=50, m=10, keyword_embeddings=None):\n",
    "    # Function to convert a URL into an embedding\n",
    "    def url_to_embedding(url):\n",
    "        # Clean the URL by keeping only alphanumeric characters and spaces\n",
    "        cleaned_url = re.sub(r\"[^a-zA-Z\\s]\", \"\", url)\n",
    "        cleaned_url = cleaned_url.lower()\n",
    "\n",
    "        # Remove stop words\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        word_tokens = word_tokenize(cleaned_url)\n",
    "        filtered_url = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "        # Lemmatize\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_url = [lemmatizer.lemmatize(w) for w in filtered_url]\n",
    "\n",
    "        # Split the cleaned, lemmatized URL into words based on common delimiters\n",
    "        parts = lemmatized_url  # This assumes the 'parts' are now the cleaned, tokenized, and lemmatized words\n",
    "        words = [word for part in parts for word in part.split(\"-\")]\n",
    "\n",
    "        # Filter out words to only include meaningful ones (based on glove_model availability)\n",
    "        words = [word for word in words if word and word in glove_model]\n",
    "\n",
    "        # Convert words into embeddings and average them to get the URL embedding\n",
    "        if words:\n",
    "            embeddings = np.array([glove_model[word] for word in words])\n",
    "            return embeddings.mean(axis=0)\n",
    "        else:\n",
    "            # Return a zero vector if no meaningful words are found\n",
    "            return np.zeros((glove_model.vector_size,))\n",
    "\n",
    "    # Function to filter URLs for a single company\n",
    "    def filter_for_company(group):\n",
    "        if len(group) <= n:\n",
    "            return group\n",
    "        else:\n",
    "            # Convert URLs to embeddings\n",
    "            url_embeddings = np.array(\n",
    "                [url_to_embedding(url) for url in group[\"src_url\"]]\n",
    "            )\n",
    "\n",
    "            # Calculate similarity to keywords\n",
    "            similarities = cosine_similarity(url_embeddings, keyword_embeddings).mean(\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            # Get indices of the top 'm' similar URLs\n",
    "            top_indices = np.argsort(similarities)[-m:]\n",
    "\n",
    "            return group.iloc[top_indices]\n",
    "\n",
    "    # Apply the filter to each company\n",
    "    filtered_df = df.groupby(\"ascii_id_company\", group_keys=False).apply(\n",
    "        filter_for_company\n",
    "    )\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filter_urls_by_similarity(en_filtered_df, 60, 60, bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for instance tesla which had issues before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df[\"ascii_id_company\"] == \"bWyO7uUNWBS9MN2QvXHLzQ==\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in df[\"src_url\"][df[\"ascii_id_company\"] == \"bWyO7uUNWBS9MN2QvXHLzQ==\"]:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in df[\"src_url\"][df[\"ascii_id_company\"] == \"bWyO7uUNWBS9MN2QvXHLzQ==\"]:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in df[\"src_url\"][df[\"ascii_id_company\"] == \"+xs/sbiUV1CWPJtfrGWtMw==\"]:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in df[\"src_url\"][df[\"ascii_id_company\"] == \"+xs/sbiUV1CWPJtfrGWtMw==\"]:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in df.iloc[:50][\"src_url\"]:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urls_filtered = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store df_urls_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ascii_rs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

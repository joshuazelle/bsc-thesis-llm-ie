{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sacremoses #for hugging face models\n",
    "# pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# dev_mode = True\n",
    "dev_mode = False\n",
    "if dev_mode:\n",
    "    # DEV (user specific)\n",
    "    database = \"/home/heiler/development/projects/ascii/research-space/src/pipelines/ascii/ascii_dbt/ascii_pipeline.duckdb\"\n",
    "    prefix = \"ascii_dev\"\n",
    "else:\n",
    "    # prod\n",
    "    database = \"/data/raid5/data/ascii/mastered-data/ascii_pipeline.duckdb\"\n",
    "    prefix = \"ascii\"\n",
    "\n",
    "con = duckdb.connect(\n",
    "    database=database,\n",
    "    read_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r content_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = content_df\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now append all the html content into one large html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'ascii_id_company' and aggregate the other columns\n",
    "df_grouped = (\n",
    "    df.groupby(\"ascii_id_company\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"src_url\": lambda x: list(x),  # Convert all src_url values to a list\n",
    "            \"content\": lambda x: \" \".join(x),  # Join all content values with a space\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Rename columns to match your requirement\n",
    "df_grouped = df_grouped.rename(\n",
    "    columns={\"src_url\": \"url_list\", \"content\": \"content_combined\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract text from html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_text_simple(html_content):\n",
    "    soup = BeautifulSoup(html_content, \"lxml\")\n",
    "\n",
    "    # Extracting text from paragraph and article tags only\n",
    "    text = \" \".join(\n",
    "        [\n",
    "            element.get_text(separator=\" \", strip=True)\n",
    "            for element in soup.find_all([\"p\", \"article\"])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'content_combined' column to create 'extr_text'\n",
    "df_grouped[\"extr_text\"] = df_grouped[\"content_combined\"].apply(extract_text_simple)\n",
    "\n",
    "# Now df_grouped has an additional column 'extr_text' with the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "def display_text_as_markdown(text):\n",
    "    if len(text) < 10000:\n",
    "        # Convert the text to a Markdown formatted string (using triple backticks for code block)\n",
    "        formatted_text = f\"```{text}```\"\n",
    "        # Display the text as Markdown, which will show it in a preformatted style\n",
    "        display(Markdown(formatted_text))\n",
    "    else:\n",
    "        print(f\"over 10k chars ({len(text)}), but here are the first 10k: \\n\")\n",
    "        text = text[:10000]\n",
    "        # Convert the text to a Markdown formatted string (using triple backticks for code block)\n",
    "        formatted_text = f\"```{text}```\"\n",
    "        # Display the text as Markdown, which will show it in a preformatted style\n",
    "        display(Markdown(formatted_text))\n",
    "\n",
    "\n",
    "# Example usage with the first row's extracted text\n",
    "display_text_as_markdown(df_grouped.iloc[0][\"extr_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_text_as_markdown(df_grouped.iloc[16][\"extr_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now check how long the texts are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute lengths of the extracted texts\n",
    "text_lengths = df_grouped[\"extr_text\"].apply(len)\n",
    "\n",
    "# Calculate basic statistics\n",
    "min_length = text_lengths.min()\n",
    "max_length = text_lengths.max()\n",
    "median_length = text_lengths.median()\n",
    "average_length = text_lengths.mean()\n",
    "\n",
    "# Print the statistics\n",
    "print(f\"Minimum length: {min_length}\")\n",
    "print(f\"Maximum length: {max_length}\")\n",
    "print(f\"Median length: {median_length}\")\n",
    "print(f\"Average length: {average_length:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check longest ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df_grouped' has columns 'id' and 'extr_text'\n",
    "# First, add a new column to your DataFrame with the text lengths\n",
    "df_grouped[\"text_length\"] = df_grouped[\"extr_text\"].apply(len)\n",
    "\n",
    "# Now, sort the DataFrame by 'text_length' in descending order to get the longest texts at the top\n",
    "df_sorted = df_grouped.sort_values(by=\"text_length\", ascending=False)\n",
    "\n",
    "# Finally, print the top 10 rows with the longest texts\n",
    "print(df_sorted[[\"ascii_id_company\", \"text_length\"]].head(15))\n",
    "\n",
    "longest_text = df_sorted[\"ascii_id_company\"].head(15).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_text_as_markdown(df_grouped.iloc[87][\"extr_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity search in extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.downloader import load\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Load the GloVe model\n",
    "glove_model = load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "# Define your keywords from before\n",
    "keyword_embeddings = bow\n",
    "\n",
    "\n",
    "def clean_and_tokenize(text, glove_model):\n",
    "    # Lowercase and remove non-alphanumeric characters\n",
    "    text = re.sub(r\"\\W+\", \" \", text.lower())\n",
    "    # Tokenize and filter tokens not in GloVe and stopwords\n",
    "    tokens = [\n",
    "        word\n",
    "        for word in word_tokenize(text)\n",
    "        if word in glove_model and word not in stop_words\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def text_to_chunks(tokens, chunk_size=500):\n",
    "    # Divide tokens into chunks of specified size\n",
    "    for i in range(0, len(tokens), chunk_size):\n",
    "        yield tokens[i : i + chunk_size]\n",
    "\n",
    "\n",
    "def chunk_to_embedding(chunk, glove_model):\n",
    "    # Convert chunk tokens to embeddings and calculate mean embedding\n",
    "    embeddings = np.array([glove_model[token] for token in chunk])\n",
    "    return embeddings.mean(axis=0)\n",
    "\n",
    "\n",
    "def filter_chunks_by_similarity(\n",
    "    df,\n",
    "    glove_model,\n",
    "    keyword_embeddings,\n",
    "    n_chunks=5,\n",
    "    chunk_size=250,\n",
    "    min_text_length=10000,\n",
    "):\n",
    "    filtered_texts = []\n",
    "\n",
    "    for text in df[\"extr_text\"]:\n",
    "        if len(text) > min_text_length:\n",
    "            tokens = clean_and_tokenize(text, glove_model)\n",
    "            chunks = list(text_to_chunks(tokens, chunk_size))\n",
    "\n",
    "            if chunks:  # Proceed only if there are chunks\n",
    "                chunk_embeddings = np.array(\n",
    "                    [chunk_to_embedding(chunk, glove_model) for chunk in chunks]\n",
    "                )\n",
    "                similarities = cosine_similarity(\n",
    "                    chunk_embeddings, keyword_embeddings\n",
    "                ).mean(axis=1)\n",
    "                top_indices = np.argsort(similarities)[-n_chunks:]\n",
    "\n",
    "                # Join the top chunks into a single string\n",
    "                top_chunks = [\" \".join(chunks[index]) for index in top_indices]\n",
    "                filtered_text = \" \".join(top_chunks)\n",
    "            else:\n",
    "                filtered_text = \"\"  # Empty string if no valid chunks\n",
    "\n",
    "        else:\n",
    "            filtered_text = text\n",
    "\n",
    "        filtered_texts.append(filtered_text)\n",
    "\n",
    "    # Update the DataFrame with filtered texts\n",
    "    df[\"filtered_text\"] = filtered_texts\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_grouped is your DataFrame containing the 'extr_text' column\n",
    "df_simsearch = filter_chunks_by_similarity(\n",
    "    df_sorted, glove_model, keyword_embeddings, n_chunks=5, chunk_size=400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backup = df_simsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simsearch = df_simsearch.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_text_as_markdown(df_simsearch[\"filtered_text\"].iloc[6])  # looks ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garbage\n",
    "Ok after inspecting the top rows, drop some that were garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simsearch.drop([0, 4, 6], axis=\"index\", inplace=True)  # drop garbage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display the short texts that is where sth has not worked properly and also drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simsearch[\"text_length\"].tail(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop when text is less than 200 chars\n",
    "\n",
    "df_dropped = df_simsearch[df_simsearch[\"text_length\"] >= 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check a few of the short texts\n",
    "for i in range(len(df_dropped) - 20, len(df_dropped)):\n",
    "    print(display_text_as_markdown(df_dropped.iloc[i][\"extr_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check text from middle length\n",
    "\n",
    "display_text_as_markdown(df_dropped.iloc[14][\"filtered_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems\n",
    "\n",
    "- [ ] a lot of chinese japenese websites\n",
    "\n",
    "- relatively low quality text for the lowest few\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.drop(\n",
    "    [\"content_combined\", \"url_list\", \"extr_text\", \"index\", \"text_length\"],\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped[\"filtered_text\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extr_text = df_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extr_text = df_dropped.rename(columns={\"filtered_text\": \"extr_text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extr_text.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store df_extr_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation\n",
    "\n",
    "these were first tries and are suspended for now. That is turned into markdown cells. so just ignore this next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# switch to gpu if possible\n",
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare hugging face\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get Hugging Face API key from environment\n",
    "hf_api_key = os.getenv('HUGGINGFACE_API_KEY')\n",
    "\n",
    "# Ensure the API key is loaded\n",
    "if hf_api_key is None:\n",
    "    raise ValueError(\"Hugging Face API key not found. Make sure it's set in your .env file as HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# # Load the tokenizer and model using the API key with the updated argument\n",
    "# tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\", token=hf_api_key)\n",
    "# model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\", token=hf_api_key)\n",
    "\n",
    "# # Updated translate function\n",
    "# def translate(text):\n",
    "#     model_inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "#     translation = model.generate(**model_inputs, max_length=512, num_beams=4, early_stopping=True)\n",
    "#     translated_text = tokenizer.decode(translation[0], skip_special_tokens=True)\n",
    "#     return translated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage\n",
    "translated_text = translate(\"示例文本\")  # \"示例文本\" means example text\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def chunk_text_by_sentence(text, max_length=512):\n",
    "    sentences = sent_tokenize(text)\n",
    "    current_chunk = []\n",
    "    chunks = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(' '.join(current_chunk) + ' ' + sentence) > max_length:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def translate_text_chunked(text):\n",
    "    chunks = chunk_text_by_sentence(text)\n",
    "    translated_chunks = [translate(chunk) for chunk in chunks]\n",
    "    return ' '.join(translated_chunks)\n",
    "\n",
    "def translate_non_english(row):\n",
    "    if row['language'] != 'en':\n",
    "        return translate_text_chunked(row['extr_text'])\n",
    "    else:\n",
    "        return row['extr_text']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the translation to the first 20 rows of df_grouped\n",
    "df_trans = df_grouped.head(20).copy()\n",
    "df_trans['extr_text_en'] = df_trans.apply(translate_non_english, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display_text_as_markdown(df_trans.iloc[10]['extr_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display_text_as_markdown(df_trans.iloc[10]['extr_text_en'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display_text_as_markdown(df_trans.iloc[8]['extr_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display_text_as_markdown(df_trans.iloc[8]['extr_text_en'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test other model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "# # Initialize the tokenizer\n",
    "# tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_1.2B\")\n",
    "\n",
    "# # Load the model and move it to the GPU\n",
    "# model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_1.2B\").to(device)\n",
    "\n",
    "# def translate_m2m100(text, src_lang):\n",
    "#     tokenizer.src_lang = src_lang\n",
    "#     # Encode the text and move the tensors to the same device as the model\n",
    "#     encoded_input = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "#     # Generate translation and move the output tensors back to CPU for decoding, if needed\n",
    "#     translated_tokens = model.generate(**encoded_input).cpu()\n",
    "    \n",
    "#     # Decode the translated tokens\n",
    "#     translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "#     return translated_text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate a sample text from German to English\n",
    "sample_text_german = \"Dies ist ein Beispieltext.\"\n",
    "translated_text = translate_m2m100(sample_text_german, \"de\")\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adjust functions for new model\n",
    "\n",
    "def translate_text_chunked(text, lang):\n",
    "    if lang=='zh-cn':\n",
    "        lang = 'zh'\n",
    "    chunks = chunk_text_by_sentence(text)\n",
    "    translated_chunks = [translate_m2m100(chunk, lang) for chunk in chunks]\n",
    "    return ' '.join(translated_chunks)\n",
    "\n",
    "def translate_non_english(row):\n",
    "    if row['language'] != 'en':\n",
    "        return translate_text_chunked(row['extr_text'],row['language'])\n",
    "    else:\n",
    "        return row['extr_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_trans1 = df_grouped.head(20).copy()\n",
    "df_trans1['extr_text_en'] = df_trans1.apply(translate_non_english, axis=1) # wow gpu so much faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_trans1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display_text_as_markdown(df_trans1.iloc[8]['extr_text'])\n",
    "display_text_as_markdown(df_trans1.iloc[8]['extr_text_en'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ascii_rs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM: try OpenAI's gpt models\n",
    "\n",
    "Here we test the classification task now with the openai llm gpt-4 with their api and function calling using langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_openai_fn_runnable,\n",
    "    create_structured_output_chain,\n",
    "    create_structured_output_runnable,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df1 # from classification.ipynb\n",
    "%store -r df # from consolidate_classes.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = {\n",
    "    \"title\": \"Company\",\n",
    "    \"description\": \"Identifying information about activities of a company based on its description.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"category\": {\n",
    "            \"title\": \"Company's Category\",\n",
    "            \"description\": \"The companies predicted category, it should be part one of the following values: ['Assembly, Packaging & Interconnects','Lithography, Photomasks & Imaging', 'Logic Chip Design & Software','Material & Wafer Fabrication', 'Planarization, Inspection & Metrology'] \",\n",
    "            \"type\": \"string\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"category\"],\n",
    "}\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an supply chain expert that classifies semiconductor companies based on their activities.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the description of a semiconductor company. Classify the firm based on its activity into one of these categories of the value chain: {input}\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Tip: Make sure to answer in the correct format, only return these values: ['Assembly, Packaging & Interconnects','Lithography, Photomasks & Imaging', 'Logic Chip Design & Software','Material & Wafer Fabrication', 'Planarization, Inspection & Metrology'] A firm can be part of up to 2 classes at the same time, seperated with semicolon;.\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new runnable for classification\n",
    "runnable = create_structured_output_runnable(json_schema, llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for running and saving results\n",
    "\n",
    "# Initialize an empty list to store the results outside the function\n",
    "results = []\n",
    "\n",
    "\n",
    "def run(start_index, end_index):\n",
    "    for i in range(start_index, end_index):\n",
    "        # Retrieve the company name for the current index i\n",
    "        name_i = df1[\"georgetown_name\"].to_list()[i]\n",
    "        # Retrieve the description for the current index i\n",
    "        desc = df1[\"firm_descript\"].to_list()[i]\n",
    "        # Run the model prediction using the description\n",
    "        category_dict = runnable.invoke({\"input\": desc})\n",
    "        # Extract the category from the dictionary\n",
    "        category = category_dict.get(\n",
    "            \"category\", \"Unknown\"\n",
    "        )  # Default to 'Unknown' if 'category' key not found\n",
    "        # Append the company name and its predicted category to the results list\n",
    "        results.append((name_i, category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function for the first set of indices\n",
    "\n",
    "#### ACTION #### here is where the action happens\n",
    "\n",
    "run(0, len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once all desired partial jobs are done, convert the results list to a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=[\"georgetown_name\", \"predicted_class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually inspect values\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "def evaluate_predictions(df1, results_df):\n",
    "    # Create a temporary copy of df1\n",
    "    df_temp = df1.copy()\n",
    "\n",
    "    # Specify class columns, excluding non-class columns\n",
    "    class_columns = df_temp.columns.difference(\n",
    "        [\"georgetown_name\", \"firm_descript\", \"firm_descript_processed\"]\n",
    "    )\n",
    "\n",
    "    # Initialize MultiLabelBinarizer\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "    # Fit the MultiLabelBinarizer with the class labels\n",
    "    mlb.fit([class_columns])\n",
    "\n",
    "    # # Convert class columns to binary (0 or 1)\n",
    "    # for column in class_columns:\n",
    "    #     df_temp[column] = pd.to_numeric(df_temp[column], errors='coerce').fillna(0)\n",
    "    #     df_temp[column] = df_temp[column].apply(lambda x: 1 if x >= 1 else 0)\n",
    "\n",
    "    # Inverse transform the binary matrix to get class labels\n",
    "    gt_classes = mlb.inverse_transform(df_temp[class_columns].values)\n",
    "\n",
    "    # Add the real class labels back to df_temp\n",
    "    df_temp[\"real_classes\"] = [list(classes) for classes in gt_classes]\n",
    "\n",
    "    # Ensure predicted_classes is a list\n",
    "    results_df[\"predicted_classes\"] = results_df[\"predicted_class\"].apply(\n",
    "        lambda x: x.split(\";\")\n",
    "    )\n",
    "\n",
    "    # Join the DataFrames on 'georgetown_name'\n",
    "    joined_df = pd.merge(\n",
    "        df_temp[[\"georgetown_name\", \"real_classes\"]],\n",
    "        results_df[[\"georgetown_name\", \"predicted_classes\"]],\n",
    "        on=\"georgetown_name\",\n",
    "        how=\"inner\",\n",
    "    )\n",
    "\n",
    "    # Determine complete and partial correctness\n",
    "    joined_df[\"correct\"] = joined_df.apply(\n",
    "        lambda row: set(row[\"real_classes\"]) == set(row[\"predicted_classes\"]), axis=1\n",
    "    )\n",
    "    joined_df[\"partially_correct\"] = joined_df.apply(\n",
    "        lambda row: bool(set(row[\"real_classes\"]) & set(row[\"predicted_classes\"])),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Return the joined DataFrame for inspection\n",
    "    return joined_df\n",
    "\n",
    "\n",
    "# Usage example\n",
    "joined_df = evaluate_predictions(df1, results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MultiLabelBinarizer (globally)\n",
    "\n",
    "# Specify class columns, excluding non-class columns\n",
    "class_columns = df1.columns.difference(\n",
    "    [\"georgetown_name\", \"firm_descript\", \"firm_descript_processed\"]\n",
    ")\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Fit the MultiLabelBinarizer with the class labels\n",
    "mlb.fit([class_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def evaluate_predictions_with_metrics(df1, results_df):\n",
    "    # Use the existing evaluate_predictions function to get the joined DataFrame\n",
    "    joined_df = evaluate_predictions(df1, results_df)\n",
    "\n",
    "    # Initialize lists to store metrics for each row\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_list = []\n",
    "\n",
    "    # Loop through each row in joined_df\n",
    "    for index, row in joined_df.iterrows():\n",
    "        # Convert real and predicted classes to binary format for sklearn metrics\n",
    "        y_true = [1 if cls in row[\"real_classes\"] else 0 for cls in mlb.classes_]\n",
    "        y_pred = [1 if cls in row[\"predicted_classes\"] else 0 for cls in mlb.classes_]\n",
    "\n",
    "        # Calculate precision, recall, and F1 for this row\n",
    "        precision = precision_score(y_true, y_pred, average=\"binary\")\n",
    "        recall = recall_score(y_true, y_pred, average=\"binary\")\n",
    "        f1 = f1_score(y_true, y_pred, average=\"binary\")\n",
    "\n",
    "        # Append the metrics to their respective lists\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "\n",
    "    # Add the metrics to the joined_df\n",
    "    joined_df[\"precision\"] = precision_list\n",
    "    joined_df[\"recall\"] = recall_list\n",
    "    joined_df[\"f1\"] = f1_list\n",
    "\n",
    "    # Return the updated DataFrame\n",
    "    return joined_df\n",
    "\n",
    "\n",
    "# Usage example\n",
    "# evaluation_df = evaluate_predictions_with_metrics(df1, results_df)\n",
    "# evaluation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_model_metrics(df1, results_df):\n",
    "    # Use the existing evaluate_predictions function to get the joined DataFrame\n",
    "    joined_df = evaluate_predictions(df1, results_df)\n",
    "\n",
    "    # Initialize lists to store all true and predicted labels\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    # Loop through each row in joined_df to populate the lists\n",
    "    for index, row in joined_df.iterrows():\n",
    "        # Convert real and predicted classes to binary format for sklearn metrics\n",
    "        y_true = [1 if cls in row[\"real_classes\"] else 0 for cls in mlb.classes_]\n",
    "        y_pred = [1 if cls in row[\"predicted_classes\"] else 0 for cls in mlb.classes_]\n",
    "\n",
    "        # Append the binary lists to the aggregated lists\n",
    "        all_y_true.append(y_true)\n",
    "        all_y_pred.append(y_pred)\n",
    "\n",
    "    # Convert lists of lists to 2D numpy arrays\n",
    "    all_y_true = np.array(all_y_true)\n",
    "    all_y_pred = np.array(all_y_pred)\n",
    "\n",
    "    # Calculate global precision, recall, and F1 score\n",
    "    precision = precision_score(all_y_true, all_y_pred, average=\"micro\")\n",
    "    recall = recall_score(all_y_true, all_y_pred, average=\"micro\")\n",
    "    f1 = f1_score(all_y_true, all_y_pred, average=\"micro\")\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "# Usage example\n",
    "precision, recall, f1 = evaluate_model_metrics(df1, results_df)\n",
    "print(\n",
    "    f\"Precision: {round(precision, 2)} \\nRecall: {round(recall, 2)} \\nF1 Score: {round(f1, 2)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Notes\n",
    "So here we are using the `average='micro'` because it is more compatible with out problem with class inbalance, whereas the 'macro' argument would count all classes as equal, thereby not taking imbalances into account. In fact, in our example changing the variable to 'macro' would even give an error.\n",
    "\n",
    "Also what is a true positive? In this case every binary prediction is evaluated independently. That is, there is no notion of 'partial correctness' as in the column of the manual inspection before. So if for one company the model predicts 2 classes and only one of them is correct, it will just take these predictions as independent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the counts of completely correct predictions\n",
    "complete_correct_counts = joined_df[\"correct\"].value_counts()\n",
    "\n",
    "# Calculate the counts of partially correct predictions\n",
    "# For partially correct, consider only those instances that are not completely correct\n",
    "partially_correct_counts = joined_df.loc[\n",
    "    ~joined_df[\"correct\"], \"partially_correct\"\n",
    "].value_counts()\n",
    "\n",
    "# Print counts and percentages\n",
    "print(\"Completely Correct Predictions:\")\n",
    "print(complete_correct_counts)\n",
    "print(\n",
    "    \"Percentage of Completely Correct Predictions: {:.2f}%\".format(\n",
    "        100 * complete_correct_counts[True] / len(joined_df)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nPartially Correct Predictions (excluding completely correct ones):\")\n",
    "print(partially_correct_counts)\n",
    "if True in partially_correct_counts:\n",
    "    print(\n",
    "        \"Percentage of Partially Correct Predictions: {:.2f}%\".format(\n",
    "            100 * partially_correct_counts[True] / len(joined_df)\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(\"Percentage of Partially Correct Predictions: 0.00%\")\n",
    "\n",
    "# Calculate and print the percentage of completely incorrect predictions\n",
    "# These are instances that are not partially correct and not completely correct\n",
    "completely_incorrect_percentage = (\n",
    "    100\n",
    "    * (\n",
    "        len(joined_df)\n",
    "        - complete_correct_counts[True]\n",
    "        - partially_correct_counts.get(True, 0)\n",
    "    )\n",
    "    / len(joined_df)\n",
    ")\n",
    "print(\n",
    "    \"\\nCompletely Incorrect Predictions Percentage: {:.2f}%\".format(\n",
    "        completely_incorrect_percentage\n",
    "    )\n",
    ")\n",
    "\n",
    "# calculate the partial correct in total\n",
    "print(\n",
    "    \"\\nAt least one correct prediction: {:.2f}%\".format(\n",
    "        100 - completely_incorrect_percentage\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for gpt-3.5-turbo:\n",
    "Completely Correct Predictions:\n",
    "correct\n",
    "False    96\n",
    "True     44\n",
    "Name: count, dtype: int64\n",
    "Percentage of Completely Correct Predictions: 31.43%\n",
    "\n",
    "Partially Correct Predictions (excluding completely correct ones):\n",
    "partially_correct\n",
    "True     63\n",
    "False    33\n",
    "Name: count, dtype: int64\n",
    "Percentage of Partially Correct Predictions: 45.00%\n",
    "\n",
    "Completely Incorrect Predictions Percentage: 23.57%\n",
    "\n",
    "At least one correct prediction: 76.43%\n",
    "\n",
    "#### metrics gpt-3.5-turbo:\n",
    "\n",
    "time: 2m30s\n",
    "\n",
    "cost: 0.20 USD\n",
    "\n",
    "Precision: 0.54 \n",
    "\n",
    "Recall: 0.72 \n",
    "\n",
    "F1 Score: 0.62\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for gpt-4\n",
    "\n",
    "Completely Correct Predictions:\n",
    "correct\n",
    "False    72\n",
    "True     68\n",
    "Name: count, dtype: int64\n",
    "Percentage of Completely Correct Predictions: 48.57%\n",
    "\n",
    "Partially Correct Predictions (excluding completely correct ones):\n",
    "partially_correct\n",
    "False    44\n",
    "True     28\n",
    "Name: count, dtype: int64\n",
    "Percentage of Partially Correct Predictions: 20.00%\n",
    "\n",
    "Completely Incorrect Predictions Percentage: 31.43%\n",
    "\n",
    "At least one correct prediction: 68.57%\n",
    "\n",
    "\n",
    "### metrics gpt-4\n",
    "time: 11m30s\n",
    "\n",
    "cost: 2.00 USD\n",
    "\n",
    "Precision: 0.68 \n",
    "\n",
    "Recall: 0.63 \n",
    "\n",
    "F1 Score: 0.65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ascii_rs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

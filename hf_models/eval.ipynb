{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result hf files\n",
    "mistral_cc_0 = \"/home/zelle/development/projects/ascii/reference-data/data_raw_direct_source_drop/joshua/llm_data/mistral_0.pickle\"\n",
    "mistral_orbis_1 = \"/home/zelle/development/projects/ascii/reference-data/data_raw_direct_source_drop/joshua/llm_data/mistral_orbis_1.pickle\"\n",
    "CatPPT_orbis_0 = \"/home/zelle/development/projects/ascii/reference-data/data_raw_direct_source_drop/joshua/llm_data/CatPPT_orbis_0.pickle\"\n",
    "mistral_cc_1 = \"/home/zelle/development/projects/ascii/reference-data/data_raw_direct_source_drop/joshua/llm_data/mistral_cc_1.pickle\"\n",
    "mistral_cc_2 = \"/home/zelle/development/projects/ascii/reference-data/data_raw_direct_source_drop/joshua/llm_data/mistral_cc_2.pickle\"\n",
    "CatPPT_cc_0 = \"/home/zelle/development/projects/ascii/reference-data/data_raw_direct_source_drop/joshua/llm_data/CatPPT_cc_0.pickle\"\n",
    "\n",
    "# gpt files\n",
    "\n",
    "gpt35_orbis_0 = \"/home/zelle/development/projects/ascii/reference-data/data_raw_direct_source_drop/joshua/llm_data/run_gpt35-turbo_orbis_0.pickle\"\n",
    "gpt35_cc_0 = \"/home/zelle/development/projects/ascii/reference-data/data_raw_direct_source_drop/joshua/llm_data/gpt35_turbo_cc_0.pickle\"\n",
    "gpt4_orbis_0 = \"/home/zelle/development/projects/ascii/reference-data/data_raw_direct_source_drop/joshua/llm_data/gpt4_orbis_0.pickle\"\n",
    "gpt35_inst_orbis_0 = \"/home/zelle/development/projects/ascii/reference-data/data_raw_direct_source_drop/joshua/llm_data/gpt35_instruct_orbis_0.pickle\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the LLMs performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that just prints results\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def evaluate(outcome_file):\n",
    "    df = pd.read_pickle(outcome_file)\n",
    "    # Ensure all entries in 'class' and 'predicted' are lists\n",
    "    df[\"class\"] = df[\"class\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    df[\"predicted\"] = df[\"predicted\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "    # Initialize MultiLabelBinarizer\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "    # Fit the MultiLabelBinarizer on the 'class' column only, assuming 'class' and 'predicted' have the same classes\n",
    "    mlb.fit(df[\"class\"])\n",
    "\n",
    "    # Transform 'class' and 'predicted' columns into binary format\n",
    "    y_true = mlb.transform(df[\"class\"])\n",
    "    y_pred = mlb.transform(df[\"predicted\"])\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = precision_score(y_true, y_pred, average=\"micro\")\n",
    "    recall = recall_score(y_true, y_pred, average=\"micro\")\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "\n",
    "    # Print the scores\n",
    "    print(f\"Precision: {round(precision, 2)}\")\n",
    "    print(f\"Recall: {round(recall, 2)}\")\n",
    "    print(f\"F1 Score: {round(f1, 2)}\")\n",
    "\n",
    "    # Count the number of exact matches\n",
    "    exact_matches = sum(\n",
    "        [\n",
    "            set(actual) == set(predicted)\n",
    "            for actual, predicted in zip(df[\"class\"], df[\"predicted\"])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Calculate the exact match ratio by dividing the number of exact matches by the total number of instances\n",
    "    exact_match_ratio = exact_matches / len(df)\n",
    "\n",
    "    print(f\"Exact matches: {exact_matches} out of {len(df)}\")\n",
    "    print(f\"Exact Match Ratio: {round(exact_match_ratio, 2)}\")\n",
    "\n",
    "    # Count the number of partial matches (at least one label is correct)\n",
    "    partial_matches = sum(\n",
    "        [\n",
    "            len(set(actual) & set(predicted)) > 0\n",
    "            for actual, predicted in zip(df[\"class\"], df[\"predicted\"])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Calculate the partial match ratio by dividing the number of partial matches by the total number of instances\n",
    "    partial_match_ratio = partial_matches / len(df)\n",
    "    print(f\"Partial Matches: {partial_matches} out of {len(df)}\")\n",
    "    print(f\"Partial Match Ratio: {round(partial_match_ratio, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    \"/home/zelle/development/projects/ascii/reference-data/data_raw_direct_source_drop/joshua/llm_data/test_gollie_0.pickle\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    \"/home/zelle/development/projects/ascii/reference-data/data_raw_direct_source_drop/joshua/llm_data/mistral_cc_regex.pickle\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    \"/home/zelle/development/projects/ascii/reference-data/data_raw_direct_source_drop/joshua/llm_data/mistral_cc_2.pickle\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    \"/home/zelle/development/projects/ascii/reference-data/data_raw_direct_source_drop/joshua/llm_data/gollie_orbis_test2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    \"/home/zelle/development/projects/ascii/reference-data/data_raw_direct_source_drop/joshua/llm_data/gollie_cc_0.pickle\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that returns a result dataframe\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def evaluate(outcome_file):\n",
    "    df = pd.read_pickle(outcome_file)\n",
    "    df[\"class\"] = df[\"class\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    df[\"predicted\"] = df[\"predicted\"].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb.fit(df[\"class\"])\n",
    "    y_true = mlb.transform(df[\"class\"])\n",
    "    y_pred = mlb.transform(df[\"predicted\"])\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, average=\"micro\")\n",
    "    recall = recall_score(y_true, y_pred, average=\"micro\")\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "\n",
    "    exact_matches = sum(\n",
    "        [\n",
    "            set(actual) == set(predicted)\n",
    "            for actual, predicted in zip(df[\"class\"], df[\"predicted\"])\n",
    "        ]\n",
    "    )\n",
    "    exact_match_ratio = exact_matches / len(df)\n",
    "\n",
    "    partial_matches = sum(\n",
    "        [\n",
    "            len(set(actual) & set(predicted)) > 0\n",
    "            for actual, predicted in zip(df[\"class\"], df[\"predicted\"])\n",
    "        ]\n",
    "    )\n",
    "    partial_match_ratio = partial_matches / len(df)\n",
    "\n",
    "    # Creating DataFrame with object type to allow mixed types\n",
    "    results_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Metric\": [\n",
    "                \"Precision\",\n",
    "                \"Recall\",\n",
    "                \"F1 Score\",\n",
    "                \"Exact Matches\",\n",
    "                \"Exact Match Ratio\",\n",
    "                \"Partial Matches\",\n",
    "                \"Partial Match Ratio\",\n",
    "                \"Nr Firms\",\n",
    "            ],\n",
    "            \"Value\": [\n",
    "                round(precision, 2),  # float\n",
    "                round(recall, 2),  # float\n",
    "                round(f1, 2),  # float\n",
    "                exact_matches,  # int, stored as object\n",
    "                round(exact_match_ratio, 2),  # float\n",
    "                partial_matches,  # int, stored as object\n",
    "                round(partial_match_ratio, 2),  # float\n",
    "                len(df),  # int, stored as object\n",
    "            ],\n",
    "        },\n",
    "        dtype=object,\n",
    "    )  # Setting dtype as object\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4\n",
    "\n",
    "### on orbis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(gpt4_orbis_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT 3.5-turbo\n",
    "\n",
    "### on orbis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(gpt35_orbis_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(gpt35_cc_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral-7B-inst\n",
    "\n",
    "### on commoncrawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(mistral_cc_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on commoncrawl 2.0 (with enhanced similarity search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(mistral_cc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(mistral_cc_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## on orbis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(mistral_orbis_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatPPT\n",
    "\n",
    "model: rishiraj/CatPPT\n",
    "\n",
    "this is the top model of the [LLM-performance-leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard) with 7B parameters which is in a memory size (ca. 16GB) that we can use on our GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on orbis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(CatPPT_orbis_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would random predicition go?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_expected_results(n_rows, n_classes, prob_predict):\n",
    "    \"\"\"\n",
    "    Calculate and print expected precision, recall, F1 score, exact matches, and partial matches\n",
    "    for a completely random multi-label classification scenario.\n",
    "\n",
    "    Parameters:\n",
    "    - n_rows: int, number of instances (rows) in the dataset.\n",
    "    - n_classes: int, number of possible classes.\n",
    "    - prob_predict: float, probability that a class is predicted for an instance.\n",
    "    \"\"\"\n",
    "    # Calculations\n",
    "    expected_precision = prob_predict\n",
    "    expected_recall = prob_predict  # Assuming the probability of a relevant label being selected is the same\n",
    "    expected_f1 = (\n",
    "        2\n",
    "        * (expected_precision * expected_recall)\n",
    "        / (expected_precision + expected_recall)\n",
    "    )\n",
    "\n",
    "    # Exact Match Ratio: Probability of predicting the exact set of labels correctly by chance\n",
    "    exact_match_ratio = prob_predict**n_classes\n",
    "\n",
    "    # Partial Match Ratio: Probability of getting at least one label correct\n",
    "    partial_match_ratio = 1 - ((1 - prob_predict) ** n_classes)\n",
    "\n",
    "    # Results for N rows\n",
    "    expected_exact_matches = n_rows * exact_match_ratio\n",
    "    expected_partial_matches = n_rows * partial_match_ratio\n",
    "\n",
    "    # Print the expected results\n",
    "    print(f\"Precision: {expected_precision}\")\n",
    "    print(f\"Recall: {expected_recall}\")\n",
    "    print(f\"F1 Score: {expected_f1}\")\n",
    "    print(f\"Exact Matches: Approximately {expected_exact_matches} instances\")\n",
    "    print(f\"Partial Matches: Approximately {expected_partial_matches} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With 116 rows, 5 classes, and 0.5 probability of predicting a class\n",
    "calculate_expected_results(116, 5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With 116 rows, 5 classes, and 0.2 probability of predicting a class\n",
    "calculate_expected_results(116, 5, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we took the 80 classes from gt:\n",
    "\n",
    "calculate_expected_results(116, 80, 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make result table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Final table\n",
    "# Define the base path\n",
    "base_path = \"/home/zelle/development/projects/ascii/reference-data/data_raw_direct_source_drop/joshua/llm_data/\"\n",
    "\n",
    "# Define file paths relative to the base path\n",
    "file_paths = {\n",
    "    \"hf_models\": [\n",
    "        base_path + \"mistral_0.pickle\",\n",
    "        base_path + \"mistral_orbis_1.pickle\",\n",
    "        base_path + \"CatPPT_orbis_0.pickle\",\n",
    "        # base_path + 'mistral_cc_1.pickle',\n",
    "        base_path + \"mistral_cc_2.pickle\",\n",
    "        base_path + \"CatPPT_cc_0.pickle\",\n",
    "        base_path + \"gollie_cc_0.pickle\",\n",
    "        base_path + \"gollie_orbis_0.pickle\",\n",
    "    ],\n",
    "    \"openai_models\": [\n",
    "        base_path + \"run_gpt35-turbo_orbis_0.pickle\",\n",
    "        base_path + \"gpt35_turbo_cc_0.pickle\",\n",
    "        base_path + \"gpt4_orbis_0.pickle\",\n",
    "        # base_path + 'gpt35_instruct_orbis_0.pickle'\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame for concatenated results\n",
    "concatenated_results = pd.DataFrame()\n",
    "\n",
    "# Iterate over all files, evaluate, and concatenate\n",
    "for category, files in file_paths.items():\n",
    "    for file in files:\n",
    "        file_path = file\n",
    "        result_df = evaluate(file_path)\n",
    "\n",
    "        # Rename 'Value' column to the file name without '.pickle' extension\n",
    "        result_df.rename(\n",
    "            columns={\"Value\": file.replace(\".pickle\", \"\").replace(base_path, \"\")},\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        # Concatenate the results\n",
    "        if concatenated_results.empty:\n",
    "            concatenated_results = result_df\n",
    "        else:\n",
    "            concatenated_results = pd.concat(\n",
    "                [concatenated_results, result_df.iloc[:, 1]], axis=1\n",
    "            )\n",
    "\n",
    "# Now concatenated_results contains all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize two empty DataFrames for cc and orbis results\n",
    "cc_results = pd.DataFrame()\n",
    "orbis_results = pd.DataFrame()\n",
    "\n",
    "# Iterate over all files, evaluate, and concatenate based on filename\n",
    "for category, files in file_paths.items():\n",
    "    for file in files:\n",
    "        file_path = file\n",
    "        result_df = evaluate(file_path)\n",
    "\n",
    "        # Check if 'cc' is in the filename and concatenate to cc_results\n",
    "        if \"cc\" in file:\n",
    "            result_df.rename(\n",
    "                columns={\"Value\": file.replace(\".pickle\", \"\").replace(base_path, \"\")},\n",
    "                inplace=True,\n",
    "            )\n",
    "            if cc_results.empty:\n",
    "                cc_results = result_df\n",
    "            else:\n",
    "                cc_results = pd.concat([cc_results, result_df.iloc[:, 1]], axis=1)\n",
    "\n",
    "        # Check if 'orbis' is in the filename and concatenate to orbis_results\n",
    "        elif \"orbis\" in file:\n",
    "            result_df.rename(\n",
    "                columns={\"Value\": file.replace(\".pickle\", \"\").replace(base_path, \"\")},\n",
    "                inplace=True,\n",
    "            )\n",
    "            if orbis_results.empty:\n",
    "                orbis_results = result_df\n",
    "            else:\n",
    "                orbis_results = pd.concat([orbis_results, result_df.iloc[:, 1]], axis=1)\n",
    "\n",
    "# cc_results now contains all data for 'cc' files\n",
    "# orbis_results now contains all data for 'orbis' files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to pivot the cc_results DataFrame\n",
    "cc_results_pivoted = cc_results.set_index(\"Metric\").transpose().reset_index()\n",
    "cc_results_pivoted.rename(columns={\"index\": \"Model\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_orbis = orbis_results.to_latex(\n",
    "    index=False,\n",
    "    caption=\"Model Evaluation Metrics on Orbis\",\n",
    "    label=\"tab:orbis_metrics\",\n",
    "    float_format=\"%.2f\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latex_orbis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_cc = cc_results.to_latex(\n",
    "    index=False,\n",
    "    caption=\"Model Evaluation Metrics on CommonCrawl\",\n",
    "    label=\"tab:cc_metrics\",\n",
    "    float_format=\"%.2f\",\n",
    ")\n",
    "print(latex_cc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ascii_rs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

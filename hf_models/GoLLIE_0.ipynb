{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2afe3b",
   "metadata": {},
   "source": [
    "<img src=\"../assets/CoLLIE_blue.png\" alt=\"GoLLIE\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003b73a2",
   "metadata": {},
   "source": [
    "# Custom Tasks with GoLLIE\n",
    "\n",
    "This notebook is an example of how to run Custom Tasks with GoLLIE. This notebook covers:\n",
    "\n",
    "- How to define the guidelines for a task\n",
    "- How to load GoLLIE\n",
    "- How to generate model inputs\n",
    "- How to parse the output\n",
    "- How to implement a scorer and evaluate the output\n",
    "\n",
    "You can modify this notebook to run any task task you want "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b015c64",
   "metadata": {},
   "source": [
    "### Import requeriments\n",
    "\n",
    "See the requeriments.txt file in the main directory to install the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed51491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")  # Add the GoLLIE base directory to sys path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ff498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich\n",
    "import logging\n",
    "from src.model.load_model import load_model\n",
    "import black\n",
    "import inspect\n",
    "from jinja2 import Template as jinja2Template\n",
    "import tempfile\n",
    "from src.tasks.utils_typing import AnnotationList\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from typing import Dict, List, Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004626bb",
   "metadata": {},
   "source": [
    "## Load GoLLIE\n",
    "\n",
    "We will load GOLLIE-7B from the huggingface-hub.\n",
    "You can use the function AutoModelForCausalLM.from_pretrained if you prefer it. However, we provide a handy load_model function with many functionalities already implemented that will assist you in reproducing our results.\n",
    "\n",
    "Please note that setting use_flash_attention=True is mandatory. Our flash attention implementation has small numerical differences compared to the attention implementation in Huggingface. Using use_flash_attention=False will result in the model producing inferior results. Flash attention requires an available CUDA GPU. Running GOLLIE pre-trained models on a CPU is not supported. We plan to address this in future releases.\n",
    "\n",
    "- Set force_auto_device_map=True to automatically load the model on available GPUs.\n",
    "- Set quantization=4 if the model doesn't fit in your GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb841c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(\n",
    "    inference=True,\n",
    "    model_weights_name_or_path=\"HiTZ/GoLLIE-7B\",\n",
    "    quantization=None,\n",
    "    use_lora=False,\n",
    "    force_auto_device_map=True,\n",
    "    use_flash_attention=True,\n",
    "    torch_dtype=\"bfloat16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e28ee25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a662bf3",
   "metadata": {},
   "source": [
    "## Define the guidelines\n",
    "\n",
    "First, we will define the labels and guidelines for the task. We will represent them as Python classes.\n",
    "\n",
    "The following guidelines have been defined for this example. They were not part of the pre-training dataset. Therefore, we will run GOLLIE in zero-shot settings using unseen labels.\n",
    "\n",
    "We will use the `Generic` class, which is a versatile class that allows for the implementation of any task you want. However, since the model has never seen the Generic label during training, we will rename it to Template, which is recognized by the model (as it was used in the Tacred dataset).\n",
    "\n",
    "We will define two classes: `Launcher` and `Mission`. Each class will have a definition and a set of slots that the model needs to fill. Each slot also requires a type definition and a short description, which can include examples. For instance, for the `Launcher` class, we define three slots:\n",
    "\n",
    "- The `mention`, which will be the name of the Launcher vehicle and should be a string.\n",
    "- The `space_company` that operated the vehicle, which will also be a string.\n",
    "- The `crew`, which is defined as a list of astronauts. Therefore, GoLLIE will fill this slot with a list of strings.\n",
    "\n",
    "ðŸ’¡ Be creative and try to define your own guidelines to test GoLLIE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5d262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from src.tasks.utils_typing import dataclass\n",
    "from src.tasks.utils_typing import Generic as Template\n",
    "\n",
    "\"\"\"\n",
    "Entity definitions\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ValueChainStep(Template):\n",
    "    \"\"\"This refers to the position a semiconductor company has in a stylized value chain. This position is determined by the\n",
    "    company's activities, like what products or services it offers. We want to classify companies into 5 steps and 5 steps only.\n",
    "    The 5 classes are (seperated by semicolon): [material_resource; tool_resource; Chip Design; Fabrication;\n",
    "    Assembly, Testing & Packaging (ATP)].\n",
    "\n",
    "    IMPORTANT: choose AT LEAST one class. One class is always correct, sometimes more.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    predicted_steps: List[str]\n",
    "    \"\"\"\n",
    "    Here you should give the step of the semiconductor value chain that you think the company is in. Only use 5 the classes that I\n",
    "    gave you:  [material_resource; tool_resource; Chip Design; Fabrication; Assembly, Testing & Packaging (ATP)] A firm can be in more\n",
    "    than one step, and even in all steps.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "ENTITY_DEFINITIONS: List[Template] = [ValueChainStep]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cell_txt = In[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f8ef55",
   "metadata": {},
   "source": [
    "### Print the guidelines to guidelines.py\n",
    "\n",
    "Due to IPython limitations, we must write the content of the previous cell to a file and then import the content from that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4736a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"guidelines.py\", \"w\", encoding=\"utf8\") as python_guidelines:\n",
    "    print(cell_txt, file=python_guidelines)\n",
    "\n",
    "from guidelines import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac3db6f",
   "metadata": {},
   "source": [
    "We use inspect.getsource to get the guidelines as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89454475",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidelines = [inspect.getsource(definition) for definition in ENTITY_DEFINITIONS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bd26b7",
   "metadata": {},
   "source": [
    "## Define input sentence\n",
    "\n",
    "Here we define the input sentence and the gold labels.\n",
    "\n",
    "You can define and empy list as gold labels if you don't have gold annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1139c165",
   "metadata": {},
   "source": [
    "### So here I jump in. Get the text from orbis and cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf2d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle(\n",
    "    \"/home/zelle/development/projects/ascii/reference-data/data_raw_direct_source_drop/joshua/llm_data/gt_orb.pickle\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e1fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bff075",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the first text for model\n",
    "text = df[\"text\"][i]\n",
    "rich.print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb92535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"This company is mainly providing raw materials to customers world wide. It offers the highest quality and clean extraction methods.\"\n",
    "gold = [ValueChainStep(predicted_steps=df[\"class\"][i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d6b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"class\"][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90501322",
   "metadata": {},
   "source": [
    "## Filling a template\n",
    "\n",
    "We need to define a template. For this task, we will include only the class definitions and the text to be annotated. However, you can design different templates to incorporate more information (for example, event triggers, as demonstrated in the Event Extraction notebook).\n",
    "\n",
    "We will use Jinja templates, which are easy to implement and exceptionally fast. For more information, visit: https://jinja.palletsprojects.com/en/3.1.x/api/#high-level-api.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b365413",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_txt = \"\"\"# The following lines describe the task definition\n",
    "# Here is what you should identify, the segment that a semiconductor company belongs to in the value chain.\n",
    "{%- for definition in guidelines %}\n",
    "{{ definition }}\n",
    "{%- endfor %}\n",
    "\n",
    "# This is the text to analyze\n",
    "text = {{ text.__repr__() }}\n",
    "\n",
    "# Now here you should say what segments or value chain steps the company belongs to:\n",
    "result = [\n",
    "{%- for ann in annotations %}\n",
    "    {{ ann }},\n",
    "{%- endfor %}\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f54034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = jinja2Template(template_txt)\n",
    "# Fill the template\n",
    "formated_text = template.render(\n",
    "    guidelines=guidelines, text=text, annotations=gold, gold=gold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886dbc0c",
   "metadata": {},
   "source": [
    "### Black Code Formatter\n",
    "\n",
    "We use the Black Code Formatter to automatically unify all the prompts to the same format. \n",
    "\n",
    "https://github.com/psf/black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8994924",
   "metadata": {},
   "outputs": [],
   "source": [
    "black_mode = black.Mode()\n",
    "formated_text = black.format_str(formated_text, mode=black_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aa5c0e",
   "metadata": {},
   "source": [
    "### Print the filled and formatted template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5f3106",
   "metadata": {},
   "outputs": [],
   "source": [
    "rich.print(formated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6165c0d",
   "metadata": {},
   "source": [
    "## Prepare model inputs\n",
    "\n",
    "We remove everything after `result =` to run inference with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eabf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, _ = formated_text.split(\"result =\")\n",
    "prompt = prompt + \"result =\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0620be2",
   "metadata": {},
   "source": [
    "Tokenize the input sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13c79d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = tokenizer(prompt, add_special_tokens=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff91970",
   "metadata": {},
   "source": [
    "Remove the `eos` token from the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9d69ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input[\"input_ids\"] = model_input[\"input_ids\"][:, :-1]\n",
    "model_input[\"attention_mask\"] = model_input[\"attention_mask\"][:, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6718528",
   "metadata": {},
   "source": [
    "## Run GoLLIE\n",
    "\n",
    "We generate the predictions using GoLLIE.\n",
    "\n",
    "We use `num_beams=1` and `do_sample=False` in our exmperiments. But feel free to experiment with differen decoding strategies ðŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f95263",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_ouput = model.generate(\n",
    "    **model_input.to(model.device),\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    min_new_tokens=1,\n",
    "    num_beams=1,\n",
    "    num_return_sequences=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebf8973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "983f4a2a",
   "metadata": {},
   "source": [
    "### Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31808b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for y, x in enumerate(model_ouput):\n",
    "    print(f\"Answer {y}\")\n",
    "    rich.print(tokenizer.decode(x, skip_special_tokens=True).split(\"result = \")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e2013f",
   "metadata": {},
   "source": [
    "## Parse the output\n",
    "\n",
    "The output is a Python list of instances, we can execute it  ðŸ¤¯\n",
    "\n",
    "We define the AnnotationList class to parse the output with a single line of code. The `AnnotationList.from_output` function filters any label that we did not define (hallucinations) to prevent getting an `undefined class` error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d66fb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = AnnotationList.from_output(\n",
    "    tokenizer.decode(model_ouput[0], skip_special_tokens=True).split(\"result = \")[-1],\n",
    "    task_module=\"guidelines\",\n",
    ")\n",
    "rich.print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27586bd8",
   "metadata": {},
   "source": [
    "Labels are an instance of the defined classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd039309",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2703325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].predicted_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c52537",
   "metadata": {},
   "source": [
    "# Evaluate the result\n",
    "\n",
    "Finally, we will evaluate the outputs from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715535b7",
   "metadata": {},
   "source": [
    "First, we define an Scorer, for Named Entity Recognition, we will use the `SpanScorer` class.\n",
    "\n",
    "We need to define the `valid_types` for the scorer, which will be the labels that we have defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e1f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tasks.utils_scorer import TemplateScorer\n",
    "\n",
    "\n",
    "class MyScorer(TemplateScorer):\n",
    "    \"\"\"Compute the F1 score for Generic Task\"\"\"\n",
    "\n",
    "    valid_types: List[Type] = ENTITY_DEFINITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9907289",
   "metadata": {},
   "source": [
    "### Instanciate the scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d6c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = MyScorer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb11ce1",
   "metadata": {},
   "source": [
    "### Compute F1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da72e844",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer_results = scorer(reference=[gold], predictions=[result])\n",
    "rich.print(scorer_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d86d47e",
   "metadata": {},
   "source": [
    "GoLLIE has successfully labeled a sentence using a set of labels that were not part of the pretraining dataset ðŸŽ‰ðŸŽ‰ðŸŽ‰\n",
    "\n",
    "GoLLIE will perform well on labels with well-defined and clearly bounded guidelines. \n",
    "\n",
    "Please share your cool experiments with us; we'd love to see what everyone is doing with GoLLIE!\n",
    "- [@iker_garciaf](https://twitter.com/iker_garciaf)\n",
    "- [@osainz59](https://twitter.com/osainz59)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ascii_rs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

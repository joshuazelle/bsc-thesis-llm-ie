{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the data\n",
    "\n",
    "path = \"/data/raid5/data/ascii/mastered-data/reference-data/data_raw_old/orbis_georgetown/orbis_georgetown.parquet\"\n",
    "\n",
    "df1 = pd.read_parquet(path)  # to be updated/ cleaned with sql\n",
    "\n",
    "df2 = pd.read_csv(\n",
    "    \"/home/zelle/development/projects/ascii/my_dev/georgetown_orbis/ml_data_orbisgt.csv\",\n",
    "    index_col=0,\n",
    ")  # to be updated\n",
    "\n",
    "df = pd.merge(\n",
    "    df2,\n",
    "    df1[[\"georgetown_name\", \"website_address\", \"bv_d_id_number\"]],\n",
    "    on=\"georgetown_name\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "input_path = \"/home/zelle/development/projects/ascii/reference-data/data_raw_direct_source_drop/georgetown/inputs.csv\"\n",
    "\n",
    "inputs = pd.read_csv(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = df[\"provided_name\"].value_counts()\n",
    "\n",
    "df[\"type\"].unique()\n",
    "\n",
    "\n",
    "# Iterate over the Series and print each index-value pair\n",
    "for index, value in frequency.items():\n",
    "    print(f\"{index}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidation of classes\n",
    "\n",
    "we see that the classes are very sparsely populated. 80 classes with only 174 companies. Lets do consolidation of classes. Try to do it with clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_names = inputs[\"input_name\"].values\n",
    "\n",
    "input_names_cleaned = [\n",
    "    name.lower().replace(\":\", \"\").replace(\"-\", \" \") for name in input_names\n",
    "]\n",
    "input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(input_names_cleaned)  # turn input names into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of desired clusters\n",
    "n_clusters = 8\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Add cluster labels to your DataFrame\n",
    "inputs[\"cluster\"] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_clusters):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    print(inputs[inputs[\"cluster\"] == i][\"input_name\"].values, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But actually it seems more sensible to do it manually or with the help of gpt-4\n",
    "This is the manual clustering found with gpt-4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated clusters with all products included\n",
    "clusters = {\n",
    "    \"Logic Chip Design & Software\": [\n",
    "        \"Logic chip design: Advanced CPUs\",\n",
    "        \"Logic chip design: Discrete GPUs\",\n",
    "        \"Logic chip design: FPGAs\",\n",
    "        \"Logic chip design: AI ASICs\",\n",
    "        \"Electronic design automation software\",\n",
    "        \"Core intellectual property\",\n",
    "        \"Finished logic chip\",\n",
    "    ],\n",
    "    \"Material & Wafer Fabrication\": [\n",
    "        \"Crystal growing furnaces\",\n",
    "        \"Crystal machining tools\",\n",
    "        \"Wafer\",\n",
    "        \"Wafer bonding and aligning tools\",\n",
    "        \"Wafer handlers\",\n",
    "        \"Photomask handlers\",\n",
    "        \"Wafer and photomask handling\",\n",
    "        \"Ion implanters\",\n",
    "        \"Electronic gases\",\n",
    "        \"Wet chemicals\",\n",
    "    ],\n",
    "    \"Lithography, Photomasks & Imaging\": [\n",
    "        \"Advanced photolithography equipment\",\n",
    "        \"EUV scanners\",\n",
    "        \"ArF scanners\",\n",
    "        \"ArF immersion scanners\",\n",
    "        \"Photolithography\",\n",
    "        \"Maskless lithography equipment\",\n",
    "        \"Electron-beam lithography tools\",\n",
    "        \"Laser lithography tools\",\n",
    "        \"Photoresists\",\n",
    "        \"Resist processing tools\",\n",
    "        \"Advanced photomasks\",\n",
    "        \"Photomask inspection and repair tools\",\n",
    "    ],\n",
    "    \"Deposition, Layering & Thermal Processing\": [\n",
    "        \"Deposition\",\n",
    "        \"Deposition tools\",\n",
    "        \"Plasma CVD tools\",\n",
    "        \"Low-pressure CVD tools\",\n",
    "        \"High-temperature CVD tools\",\n",
    "        \"Atomic layer deposition tools\",\n",
    "        \"Physical vapor deposition tools\",\n",
    "        \"Tube-based diffusion and deposition tools\",\n",
    "        \"Electrochemical coating tools\",\n",
    "        \"Chemical vapor deposition tools\",\n",
    "        \"Deposition materials\",\n",
    "        \"Rapid thermal processing tools\",\n",
    "    ],\n",
    "    \"Etching, Cleaning & Surface Preparation\": [\n",
    "        \"Etch and clean\",\n",
    "        \"Dry etching and cleaning tools\",\n",
    "        \"Wet etching and cleaning tools\",\n",
    "        \"Conductor etching tools\",\n",
    "        \"Dielectric etching tools\",\n",
    "        \"Etching and cleaning tools\",\n",
    "    ],\n",
    "    \"Planarization, Inspection & Metrology\": [\n",
    "        \"Chemical mechanical planarization\",\n",
    "        \"Chemical mechanical planarization tools\",\n",
    "        \"Process control\",\n",
    "        \"Process monitoring equipment\",\n",
    "        \"Wafer inspection equipment\",\n",
    "        \"Wafer level packaging inspection tools\",\n",
    "        \"Film and wafer measuring tools\",\n",
    "        \"Critical dimensions measurement tools\",\n",
    "        \"Defect inspection tools\",\n",
    "        \"Assembly inspection tools\",\n",
    "        \"CMP materials\",\n",
    "    ],\n",
    "    \"Assembly, Packaging & Interconnects\": [\n",
    "        \"Assembly and packaging\",\n",
    "        \"Dicing tools\",\n",
    "        \"Bonding tools\",\n",
    "        \"Die attaching tools\",\n",
    "        \"Wire bonding tools\",\n",
    "        \"Advanced interconnect tools\",\n",
    "        \"Packaging tools\",\n",
    "        \"Integrated assembly tools\",\n",
    "        \"Handlers and probes\",\n",
    "        \"Lead frames\",\n",
    "        \"Bond wires\",\n",
    "        \"Ceramic packages\",\n",
    "        \"Substrates\",\n",
    "        \"Encapsulation resins\",\n",
    "        \"Die attach materials\",\n",
    "        \"Packaging materials\",\n",
    "    ],\n",
    "    \"Testing & Quality Assurance\": [\n",
    "        \"Testing\",\n",
    "        \"SoC test equipment\",\n",
    "        \"Burn-in test equipment\",\n",
    "        \"Linear and discrete testing tools\",\n",
    "        \"General-purpose microscopy tools\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "# Function to assign cluster based on product name\n",
    "def assign_cluster(product_name):\n",
    "    for cluster_name, products in clusters.items():\n",
    "        if product_name in products:\n",
    "            return cluster_name\n",
    "    return \"Uncategorized\"  # Fallback category, should ideally be empty\n",
    "\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "inputs[\"Cluster\"] = inputs[\"input_name\"].apply(assign_cluster)\n",
    "\n",
    "# Verify the updated clustering\n",
    "print(inputs[\"Cluster\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution now:\n",
    "\n",
    "# inputs[inputs['Cluster']=='Assembly, Packaging & Interconnects']['input_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cluster_information(df, inputs):\n",
    "    # Merge the 'df' DataFrame with the 'inputs' DataFrame to get the 'Cluster' information\n",
    "    # Use 'provided_name' from 'df' and 'input_name' from 'inputs' for merging\n",
    "    merged_df = pd.merge(\n",
    "        df,\n",
    "        inputs[[\"input_name\", \"Cluster\"]],\n",
    "        left_on=\"provided_name\",\n",
    "        right_on=\"input_name\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # Drop the extra 'input_name' column from the merge if not needed\n",
    "    merged_df.drop(\"input_name\", axis=1, inplace=True)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# Apply the function to add 'Cluster' column to 'df'\n",
    "df_with_clusters = add_cluster_information(df, inputs)\n",
    "\n",
    "# Check the first few rows to verify the 'Cluster' column is added\n",
    "df_with_clusters.head()\n",
    "\n",
    "# To count how many companies are in each cluster\n",
    "# cluster_counts = df_with_clusters['Cluster'].value_counts()\n",
    "# print(cluster_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem is that many companies may fall into more than one category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'provided_id' string on commas and explode into separate rows\n",
    "df_expanded = df.assign(provided_id=df[\"provided_id\"].str.split(\", \")).explode(\n",
    "    \"provided_id\"\n",
    ")\n",
    "\n",
    "# Remove any leading/trailing whitespace that might be left after splitting\n",
    "df_expanded[\"provided_id\"] = df_expanded[\"provided_id\"].str.strip()\n",
    "df_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'input_id' is the column in 'inputs' that corresponds to 'provided_id' in 'df_expanded'\n",
    "df_with_clusters = pd.merge(\n",
    "    df_expanded,\n",
    "    inputs[[\"input_id\", \"Cluster\"]],\n",
    "    left_on=\"provided_id\",\n",
    "    right_on=\"input_id\",\n",
    "    how=\"left\",\n",
    ")\n",
    "df_with_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'provider_id' and concatenate 'provided_id' and 'Cluster' into comma-separated strings\n",
    "df_grouped = (\n",
    "    df_with_clusters.groupby(\"provider_id\")\n",
    "    .agg({\"provided_id\": lambda x: \", \".join(x), \"Cluster\": lambda x: \"; \".join(x)})\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_grouped[[\"provider_id\", \"Cluster\"]], \"inner\", on=\"provider_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many companies have multiple clusters:\n",
    "\n",
    "# Split the 'Cluster' column on the semicolon, then explode so each cluster has its own row\n",
    "df_exploded = df.assign(Cluster=df[\"Cluster\"].str.split(\"; \")).explode(\"Cluster\")\n",
    "\n",
    "# Group by 'provider_id' and count unique 'Cluster'\n",
    "cluster_counts = df_exploded.groupby(\"georgetown_name\")[\"Cluster\"].nunique()\n",
    "\n",
    "# Sort the counts in decreasing order\n",
    "cluster_counts_sorted = cluster_counts.sort_values(ascending=False)\n",
    "cluster_counts_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing multi cluster companies?\n",
    "Now the question is if I should just remove companies that have more than one or more than 2 clusters. Maybe just run the classification problem and then decide. See how well models will perform.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only companies with up to 2 clusters\n",
    "subset = cluster_counts_sorted[cluster_counts_sorted < 3]\n",
    "\n",
    "print(\"companies remaining:\", len(subset))\n",
    "print(\"percentage kept:\", len(subset) / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check frequency\n",
    "\n",
    "# Filter 'df' to include only firms with at most 2 clusters\n",
    "df_filtered = df[df[\"georgetown_name\"].isin(subset.index)]\n",
    "\n",
    "# Split the 'Cluster' column on semicolons, then explode into separate rows\n",
    "df_clusters_exploded = df_filtered.assign(\n",
    "    Cluster=df_filtered[\"Cluster\"].str.split(\"; \")\n",
    ").explode(\"Cluster\")\n",
    "\n",
    "# Perform value count on the 'Cluster' column\n",
    "cluster_value_counts = df_clusters_exploded[\"Cluster\"].value_counts()\n",
    "\n",
    "# Display the value count frequency for the Cluster column\n",
    "cluster_value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TEST: kick out small classes:\n",
    "\n",
    "df_filtered = df\n",
    "# Define the classes you want to drop\n",
    "classes_to_drop = [\n",
    "    \"Testing & Quality Assurance\",\n",
    "    \"Etching, Cleaning & Surface Preparation\",\n",
    "    \"Deposition, Layering & Thermal Processing\",\n",
    "]\n",
    "\n",
    "\n",
    "# Function to check if any of the classes to drop are in the company's cluster\n",
    "def has_class_to_drop(cluster_string):\n",
    "    company_classes = cluster_string.split(\"; \")\n",
    "    return any(class_to_drop in company_classes for class_to_drop in classes_to_drop)\n",
    "\n",
    "\n",
    "# Apply the function to each row and filter the DataFrame\n",
    "df_filtered = df[~df[\"Cluster\"].apply(has_class_to_drop)]\n",
    "\n",
    "# test again the distribution:\n",
    "df_clusters_exploded = df_filtered.assign(\n",
    "    Cluster=df_filtered[\"Cluster\"].str.split(\"; \")\n",
    ").explode(\"Cluster\")\n",
    "\n",
    "# Perform value count on the 'Cluster' column\n",
    "cluster_value_counts = df_clusters_exploded[\"Cluster\"].value_counts()\n",
    "\n",
    "# Display the value count frequency for the Cluster column\n",
    "cluster_value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbalanced data?\n",
    "Ok so now I have this new distribution, it doesnt seem too bad yet the assembly class still looks quite big. I might want to split it up. And the last 2 classes are very small\n",
    "\n",
    "And/Or maybe I can try oversampling techniques for the actual ml task.\n",
    "\n",
    "Maybe I just try to run the model and then adjust these things where I have the immedeate feedback loop as to how the model performs given different fixes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final data preparation steps:\n",
    "- drop unnecessary columns\n",
    "- put text into one column\n",
    "- in the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_filtered\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ascii_rs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

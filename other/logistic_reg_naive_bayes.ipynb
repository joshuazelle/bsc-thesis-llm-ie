{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation for classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### column selection\n",
    "\n",
    "Now take only the `georgetown_name` as identifier, the Cluster renamed to `consolidated_input` as the predicted variable and put the text columns into one called `firm_descript`, which we will use to predict the input. Here are the columns we found the most apropriate. For instance NACE code description would be perhaps the same for some firms thus not adding variation in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the required columns\n",
    "df_selected = df[\n",
    "    [\n",
    "        \"georgetown_name\",\n",
    "        \"Cluster\",\n",
    "        \"description_and_history\",\n",
    "        \"full_overview\",\n",
    "        \"trade_description_english\",\n",
    "        \"products_services\",\n",
    "        \"main_products_and_services\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Renaming 'Cluster' to 'consolidated_input'\n",
    "df_selected = df_selected.rename(columns={\"Cluster\": \"consolidated_input\"})\n",
    "\n",
    "# Concatenating the text columns into 'firm_descript'\n",
    "df_selected[\"firm_descript\"] = df_selected[\n",
    "    [\n",
    "        \"description_and_history\",\n",
    "        \"full_overview\",\n",
    "        \"trade_description_english\",\n",
    "        \"products_services\",\n",
    "        \"main_products_and_services\",\n",
    "    ]\n",
    "].apply(lambda x: \" \".join(x.dropna()), axis=1)\n",
    "\n",
    "# Now df_selected has the columns 'georgetown_name', 'consolidated_input', and 'firm_descript'\n",
    "# You can drop the original text columns if they are no longer needed\n",
    "df = df_selected[[\"georgetown_name\", \"consolidated_input\", \"firm_descript\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now prepare the consolidated input for classification by making every class its own column and give binary values if a company is in this class or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an explicit copy of the DataFrame to avoid SettingWithCopyWarning\n",
    "df = df.copy()\n",
    "\n",
    "# Splitting the classes in 'consolidated_input' and creating a list of labels\n",
    "df[\"consolidated_input\"] = df[\"consolidated_input\"].apply(lambda x: x.split(\"; \"))\n",
    "\n",
    "# Initializing the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Transforming 'consolidated_input' into binary indicators\n",
    "df_labels = pd.DataFrame(\n",
    "    mlb.fit_transform(df[\"consolidated_input\"]), columns=mlb.classes_, index=df.index\n",
    ")\n",
    "\n",
    "# Concatenating the new binary label columns back to the original DataFrame\n",
    "df = pd.concat([df.drop(\"consolidated_input\", axis=1), df_labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "\n",
    "We use Natural Language Tool Kit (nltk) library to do some text preparation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Removing stopwords and punctuation\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# Applying preprocessing to the 'firm_descript' column\n",
    "df[\"firm_descript_processed\"] = df[\"firm_descript\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns # reorder columns\n",
    "# df = df[['georgetown_name', 'firm_descript', 'firm_descript_processed',\n",
    "#        'Assembly, Packaging & Interconnects',\n",
    "#        'Deposition, Layering & Thermal Processing',\n",
    "#        'Etching, Cleaning & Surface Preparation',\n",
    "#        'Lithography, Photomasks & Imaging', 'Logic Chip Design & Software',\n",
    "#        'Material & Wafer Fabrication', 'Planarization, Inspection & Metrology',\n",
    "#        'Testing & Quality Assurance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"unprocessed: \", df[\"firm_descript\"][0])\n",
    "print(\"processed: \", df[\"firm_descript_processed\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "Now turn this prepared text into numbers that a mathematical model can work with them. Try first the simple tfidf vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(df[\"firm_descript_processed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Machine Learning Task\n",
    "\n",
    "Now that the data is prepared we can test the first models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extracting label columns\n",
    "label_columns = df.columns.difference(\n",
    "    [\"georgetown_name\", \"firm_descript\", \"firm_descript_processed\"]\n",
    ")\n",
    "Y = df[label_columns]\n",
    "# Splitting the dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_tfidf, Y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Lets try one of the simplest models. Logistic regression. Unfortunately it doesnt seem to work at all since it predicts 0 companies correctly.\n",
    "We used the multi output classifier of logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "# Wrapping Logistic Regression in MultiOutputClassifier for multi-label classification\n",
    "model = MultiOutputClassifier(LogisticRegression(max_iter=1000), n_jobs=-1)\n",
    "\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Predictions\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem \n",
    "is that some classes are so sparsely populated that it doesnt work properly. First try to just drop some classes that are low and then see better approaches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "Since this didnt work at all lets try the Naive Bayes model, wrapped such that it can also do the multi label prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_tfidf, Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Make sure your feature values are non-negative (TF-IDF should be fine)\n",
    "# Wrapping Multinomial Naive Bayes in MultiOutputClassifier\n",
    "model_nb = MultiOutputClassifier(MultinomialNB(), n_jobs=-1)\n",
    "\n",
    "model_nb.fit(X_train, Y_train)\n",
    "\n",
    "# Predictions\n",
    "Y_pred_nb = model_nb.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(Y_test, Y_pred_nb, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "looks like the model is very conservative in predicting. Lets look at the actual probabilities to get more information, maybe we just have to tweak the threshold when a company is classified as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities\n",
    "Y_pred_probs = model_nb.predict_proba(X_test)\n",
    "Y_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets try with a different threshold, which was chosen by looking at the predicted probabilies. Given that they are quite low we choose a low 0.2 threshold.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define custom threshold\n",
    "threshold = 0.2\n",
    "\n",
    "# Initialize an empty array for custom predictions\n",
    "Y_pred_custom = np.zeros_like(Y_test)\n",
    "\n",
    "# Apply threshold to each set of probabilities\n",
    "for i, probs in enumerate(Y_pred_probs):\n",
    "    # Each probs corresponds to a classifier for one label\n",
    "    Y_pred_custom[:, i] = (probs[:, 1] >= threshold).astype(int)\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(Y_test, Y_pred_custom, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome of Naive Bayes\n",
    "\n",
    "Ok now we at least have some results. But the problem remains that there are just so few datapoints that each class in the test dataset is only represented in the order of magnitude of 10. \n",
    "\n",
    "This seems to confirm that we cannot follow this approach of training a new model from scratch. At least not a classical machnie learning model.\n",
    "\n",
    "What other options are there?\n",
    "- try llm with api\n",
    "- try fine tuning llm\n",
    "- just using embedding model and then calculating the similarity between docs, that is company description and consolidated_inputs desctiption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets take this data to new notebook\n",
    "df1 = df\n",
    "%store df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ascii_rs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
